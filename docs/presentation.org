#+title: Weather Research and Forecast (WRF) Scaling, Performance Assessment and Optimization
#+subtitle: Comparison of Compilers and MPI Libraries on Cheyenne \vspace{-.2cm}
# #+subtitle: NCAR SIParCS Program
#+date: August 3, 2018
#+author: Akira Kyle
#+email: akyle@cmu.edu
#+options: H:2 toc:t num:t author:nil
#+latex_header: \author[shortname]{\vspace{-.4cm} Akira Kyle\inst{1}, Davide Del Vento \inst{2}, Brian Vanderwende \inst{2}, Negin Sobhani \inst{2}, Dixit Patel \inst{3}}
# #+latex_header: \institute[shortinst]{\inst{1} Carnegie Mellon University \and \inst{2} National Center for Atmospheric Research \and \inst{3} University of Colorado Boulder}
#+latex_header: \titlegraphic{\begin{picture}(0,0) \put(315,-150){\makebox(0,0)[rt]{\includegraphics[width=0.25\linewidth]{Updated-SIParCS-logo.png} \includegraphics[width=0.25\linewidth]{NSF_4-Color_vector_Logo.pdf}}} \end{picture}}
#+latex_header: \institute[shortinst]{\inst{1} \includegraphics[width=0.4\linewidth]{CMU_Logo_Horiz_Red.pdf} \vspace{-.1cm} \and \inst{2} \includegraphics[width=0.34\linewidth]{ncar-logo2.pdf} \vspace{-.2cm} \and \inst{3} \includegraphics[width=0.46\linewidth]{boulder-one-line.png}}
#+latex_header: \graphicspath{{./figs/}{./images/}{./obipy-resources/}}
#+startup: beamer
#+latex_class: beamer
# #+beamer_theme: Pittsburgh
# \usecolortheme[snowy]{owl}
# #+beamer_color_theme: owl
#+beamer_theme: metropolis

#+setupfile: theme-readtheorg-local.setup
# #+html_head: <style type="text/css">body{ max-width:50em; margin-left:auto; margin-right:auto; }</style>

* Background
** The Weather Research and Forecast Model

"WRF is a state-of-the-art atmospheric modeling system designed for both
meteorological research and numerical weather prediction. It offers a host of
options for atmospheric processes and can run on a variety of computing
platforms. WRF excels in a broad range of applications across scales ranging
from tens of meters to thousands of kilometers, including the following."

#+BEAMER: \pause
– Meteorological studies

#+BEAMER: \pause
– Real-time NWP

#+BEAMER: \pause
– Idealized simulations

#+BEAMER: \pause
– Data assimilation

#+BEAMER: \pause
– Earth system model coupling

#+BEAMER: \pause
– Model training and educational support

** WRF Flowchart
#+ATTR_LATEX: :width 0.9\linewidth
[[./figs/WRF_flow_chart-ARW_v4.png]]

* Intro
** Test cases
- ~conus12km~
- ~conus2.5km~
- ~new_conus12km~
- ~new_conus2.5km~
- ~katrina1km~
- ~katrina3km~
- ~maria1km~
- ~maria3km~

** Compilers and MPI Libraries
- GNU Compiler Collection (GCC) versions 6.3.0, 8.1.0
  - WRF compiles with -O2 default
    - Tried -O3 and -mfma (enables FMA instruction set)
    - Use -ofast?
- Intel Compiler versions 17.0.1, 18.0.1
- MPT, MVAPICH

** Settings
- MVAPICH

http://mvapich.cse.ohio-state.edu/static/media/mvapich/mvapich2-2.3rc2-userguide.html#x1-19100011.15

* Results
** CONUS 12km
#+attr_latex: :width 1.0\linewidth
[[file:./imgs/conus12km.svg]]

** NEW CONUS 12km WRFV3.8.1
#+attr_latex: :width 1.0\linewidth
[[file:./imgs/new_conus12km_3.svg]]

** Old CONUS 12km vs New CONUS 12km WRFV3.8.1
#+attr_latex: :width 1.0\linewidth
[[file:./imgs/old_vs_new_conus12km_3.svg]]

** New CONUS 12km
#+attr_latex: :width 1.0\linewidth
[[file:./imgs/new_conus12km.svg]]

** New CONUS 12km Normal
#+attr_latex: :width 1.0\linewidth
[[file:./imgs/new_conus12km_bar_normal.svg]]

** New CONUS 12km Large
#+attr_latex: :width 1.0\linewidth
[[file:./imgs/new_conus12km_bar_large.svg]]

** New CONUS 2.5km
#+attr_latex: :width 1.0\linewidth
[[file:./imgs/new_conus2-5km.svg]]

** Maria 3km
#+attr_latex: :width 1.0\linewidth
[[file:./imgs/maria3km.svg]]

** Case comparison
#+attr_latex: :width 1.0\linewidth
[[file:./imgs/cases.svg]]

* WRF scaling and timing on Cheyenne :noexport:
- "Is it possible to solve a problem with such-and-such resolution in a timely
  manner?"
- "If I use more cores I will have the results more quickly, but with this
  resolution will my run be in the efficient strong-scaling regime, an
  intermediate one, or in the very inefficient one dominated by I/O and
  initialization instead of computing?"

Numbers from the figures below can help you develop some back-of-the-envelope
estimates of what will happen if you increase or decrease the core counts of
your runs, so that you can find one that is optimal for you both in terms of
time-to-solution and in terms of your allocation.

--------------------------------------------------------------------------------

Figure 1 shows scaling results from the Katrina simulations at two different
resolutions and also includes the official CONUS benchmarks from
http://www2.mmm.ucar.edu/wrf/WG2/bench/ at 12km and 2.5km resolution. For
comparison, a New Zealand case with different physics parametrization and a
higher number of vertical levels is also included. When expressed this way, all
the cases scale similarly. Note that both axes are logarithmic, so a small
distance between points corresponds to a large difference in values. Figure 1 -

#+name: case
#+caption: WRF scaling Figure 1
[[file:./imgs/cases.svg]]

As you can see, there are three regimes:

    large number of grid points per core - Total grid points / core > 105 (small
    core count) intermediate number of grid points per core - 104 < Total grid
    points / core < 105 (intermediate core count) small number of grid points
    per core - Total grid points / core < 104 (large core count)

For a small number of cores, the WRF computation kernel is in a strong scaling
regime. Increasing the core count will make the simulation go faster while it
consumes approximately the same amount of core-hours (ignoring time spent in
initialization and I/O). Time-to-solution will also depend on the wait in queue,
which may be larger for larger jobs.

For an intermediate number of cores, WRF scaling increasingly departs from
linear strong scaling. Running the same simulation on larger core counts will
require more core-hours even though it will still run faster (again, ignoring
time spent in initialization, I/O, and wait in queue).

We do not recommend running WRF on extremely large core counts, because in this
regime the speed benefits diminish, the time will be dominated by initialization
and I/O (as well as wait in queue), and there will be larger core-hours charges
for solving the same problem.

Most WRF jobs on Yellowstone use less than 4,096 cores. Run time results

Figure 2 below shows the total run time for WRF jobs using increasing numbers of
cores. Initialization time, computation time, and writing time also are shown
for runs using up to 8,192 cores. Initialization and writing time rendered
larger jobs impractical. Using a different output algorithm (CSG used the
default) may yield better results for large jobs. Compare this to Figure 1,
where only the computing time (represented in Figure 2 by the green triangles)
has been taken into account to make the plot.

These results are based on simulations of Hurricane Katrina (2005) at 1km
resolution.

As illustrated, initialization and writing output times may be more expensive
than computing time for larger core counts. Times shown are for a single restart
and single output file of the Katrina 1km case, which used a domain of about 372
million grid points. If you have more restarts and output files, your numbers
will be different but the trend will be similar. Figure 2 - WRF timing Figure 2

* Summary
** Conclusions?
- Brownian motion begins with a random walk
- $\langle R_{N}^2 \rangle = NL^2$ can be related to physical quantities through forces
  - Randomness is very helpful: it allows us to average out a terms[fn:: The Feynman Lectures on Physics, Vol. I]
