# -*- org-html-htmlize-output-type: css; -*-
#+title: Weather Research and Forecast (WRF) Scaling, Performance Assessment and Optimization
# #+subtitle: Comparison of Compilers and MPI Libraries on Cheyenne \vspace{-.2cm}
# #+subtitle: NCAR SIParCS Program
#+date: August 3, 2018
#+author: Akira Kyle
#+email: akyle@cmu.edu
#+property: header-args :results raw drawer :exports results :eval never-export
# #+options: H:2 toc:t num:t author:nil
#+options: toc:t num:nil email:t author:t date:nil
#+setupfile: theme-readtheorg-local.setup
# #+html_head: <style type="text/css">body{ max-width:50em; margin-left:auto; margin-right:auto; }</style>
#+latex_header: \author[Kyle]{\vspace{-.4cm} Akira Kyle\inst{1}, Davide Del Vento \inst{2}, Brian Vanderwende \inst{2}, Negin Sobhani \inst{2}, Dixit Patel \inst{3}}
# #+latex_header: \institute[shortinst]{\inst{1} Carnegie Mellon University \and \inst{2} National Center for Atmospheric Research \and \inst{3} University of Colorado Boulder}
#+latex_header: \titlegraphic{\begin{picture}(0,0) \put(315,-150){\makebox(0,0)[rt]{\includegraphics[width=0.25\linewidth]{Updated-SIParCS-logo.png} \includegraphics[width=0.25\linewidth]{NSF_4-Color_vector_Logo.pdf}}} \end{picture}}
#+latex_header: \institute[NCAR]{\inst{1} \includegraphics[width=0.4\linewidth]{CMU_Logo_Horiz_Red.pdf} \vspace{-.1cm} \and \inst{2} \includegraphics[width=0.34\linewidth]{ncar-logo2.pdf} \vspace{-.2cm} \and \inst{3} \includegraphics[width=0.46\linewidth]{boulder-one-line.png}}
#+latex_header: \graphicspath{{./figs/}{./images/}{./obipy-resources/}}
#+latex_class: beamer
#+beamer_theme: metropolis
# #+startup: beamer
# #+beamer_theme: Pittsburgh
# \usecolortheme[snowy]{owl}
# #+beamer_color_theme: owl

#+begin_warning
This is still very much a work in progress!
#+end_warning

* Background
** The Weather Research and Forecast Model

"WRF is a state-of-the-art atmospheric modeling system designed for both
meteorological research and numerical weather prediction. It offers a host of
options for atmospheric processes and can run on a variety of computing
platforms. WRF excels in a broad range of applications across scales ranging
from tens of meters to thousands of kilometers, including the following."

#+BEAMER: \pause
– Meteorological studies
#+BEAMER: \pause
– Real-time NWP
#+BEAMER: \pause
– Idealized simulations
#+BEAMER: \pause
– Data assimilation
#+BEAMER: \pause
– Earth system model coupling
#+BEAMER: \pause
– Model training and educational support

** WRF Flowchart
#+ATTR_LATEX: :width 0.9\linewidth
[[./figs/WRF_flow_chart-ARW_v4.png]]

* Intro
** Test cases
- ~conus12km~ and ~conus2.5km~
  - Official CONUS benchmarks from http://www2.mmm.ucar.edu/wrf/WG2/benchv3/
  - Benchmarks no longer maintained
  - Only works on WRFV3.8.1 or earlier
  - ~wrfbdy~ generated using WRFV2.2 and ~wrfrst~ generated using WRFV3.2beta
    provided
- ~katrina1km~ ~katrina3km~
  - cases based upon Katrina single domain case in WRF-ARW online tutorial
    (http://www2.mmm.ucar.edu/wrf/OnLineTutorial/CASES/SingleDomain/index.html)
  - Same domain and similar physics but higher resolution
  - cfl errors encountered often, especially with ~1km~ resolution
    - Mostly likely due to high vertical winds over mountain ranges in Mexico
- ~new_conus12km~ and ~new_conus2.5km~
  - Based on official CONUS benchmarks
  - 12km and 2.5km vertical and horizontal resolution respectively
  - Use CONUS Physics suite introduced in WRFV3.9 instead of physics used in
    above official CONUS benchmarks
  - ~cu_physics = 0~ for ~new_conus2.5~
  - NCEP GFS 0.25 deg input data for 2018-06-17
  - 6 hour run, model output at 0, 3, 6hrs, restart file at 6hrs
- ~maria3km~ and ~maria1km~
  - 3km and 1km vertical and horizontal resolution respectively
  - TROPICAL Physics suite with ~cu_physics~ disabled and ~sf_sfclay_physics = 1~

| case           | e_we | e_sn | total gridpoints | timestep |
|----------------+------+------+------------------+----------|
| conus12km      |  425 |  300 | 127,500          |       72 |
| conus2.5km     | 1901 | 1301 | 2,473,201        |       15 |
| new_conus12km  |  425 |  300 | 127,500          |       72 |
| new_conus2.5km | 1901 | 1301 | 2,473,201        |       15 |
| maria1km       | 3665 | 2894 | 10,606,510       |        3 |
| maria3km       | 1396 | 1384 | 1,932,064        |        9 |

** Compilers and MPI Libraries
- GNU Compiler Collection (GCC) versions 6.3.0, 8.1.0
  - WRF compiles with ~-O2~ by default
  - Tried ~-O3~, ~-Ofast~, ~-mfma~ (enables FMA instruction set), ~-march=native~
- Intel Compiler versions 17.0.1, 18.0.1
  - WRF compiles with ~-O3~ by default
  - Tried ~-Xhost~ which is similar to GNU's ~-march=native~
- MPT version 2.18
- MVAPICH version 2.2
- openMPI version 3.1.0
- Intel MPI version 2018.1.163
- MPICH version 3.2

* Observations:
** Compilation issue with gnu > 6.3.0 and WRF < V4.0
 This likely require a source patch to fix, not an environment or compiler issue
#+begin_example
module_cu_g3.f90:3132:41:

                    call random_seed (PUT=seed)
                                         1
Error: Size of ‘put’ argument of ‘random_seed’ intrinsic at (1) too small (12/33)
#+end_example

** ~new_conus12km~ on WRFV4.0 exceeds minimum patch size on >= 32 nodes (36 cpus per node)
#+begin_example
taskid: 0 hostname: r14i1n17
 module_io_quilt_old.F        2931 F
Quilting with   1 groups of   0 I/O tasks.
 Ntasks in X           32 , ntasks in Y           36
*************************************
Configuring physics suite 'conus'

         mp_physics:      8
         cu_physics:      6
      ra_lw_physics:      4
      ra_sw_physics:      4
     bl_pbl_physics:      2
  sf_sfclay_physics:      2
 sf_surface_physics:      2
*************************************
   For domain            1 , the domain size is too small for this many processors, or the decomposition aspect ratio is poor.
   Minimum decomposed computational patch size, either x-dir or y-dir, is 10 grid cells.
  e_we =   425, nproc_x =   32, with cell width in x-direction =   13
  e_sn =   300, nproc_y =   36, with cell width in y-direction =    8
  --- ERROR: Reduce the MPI rank count, or redistribute the tasks.
-------------- FATAL CALLED ---------------
FATAL CALLED FROM FILE:  <stdin>  LINE:    1726
NOTE:       1 namelist settings are wrong. Please check and reset these options
-------------------------------------------
#+end_example

#+begin_note
This is a change in WRFV4.0 from previous versions. From
http://www2.mmm.ucar.edu/wrf/users/wrfv4.0/updates-4.0.html:
-  "Determine the resultant x- and y-direction computational patch sizes based
  on the full domain size and the number of MPI ranks used in each direction. If
  the patch size < 10 grid cells in either direction, the model will stop."
#+end_note

** Likely not enough memory for small node count on large cases
When run on cheyenne's 64 GB memory nodes
(https://www2.cisl.ucar.edu/resources/computational-systems/cheyenne), WRFV4.0
usually see jobs SIGKILLed at various points during startup for
- new_conus2.5 on <= 2 nodes
- maria3km on 1 node
- maria1km on <= 8 nodes

** Intel takes ~2x as long to compile wrf then gnu
** WRF compilation option 66/67
Relevant diff of ~configure.wrf~:
#+begin_example
5c5
< # Compiler choice: 15
---
> # Compiler choice: 66
161,163c128,130
< ARCH_LOCAL      =       -DNONSTANDARD_SYSTEM_FUNC  -DWRF_USE_CLM $(NETCDF4_IO_OPTS)
< CFLAGS_LOCAL    =       -w -O3 -ip #-xHost -fp-model fast=2 -no-prec-div -no-prec-sqrt -ftz -no-multibyte-chars
< LDFLAGS_LOCAL   =       -ip #-xHost -fp-model fast=2 -no-prec-div -no-prec-sqrt -ftz -align all -fno-alias -fno-common
---
> ARCH_LOCAL      =       -DNONSTANDARD_SYSTEM_FUNC -DWRF_USE_CLM $(NETCDF4_IO_OPTS)
> CFLAGS_LOCAL    =       -w -O3 -ip -xHost -fp-model fast=2 -no-prec-div -no-prec-sqrt -ftz -no-multibyte-chars -xCORE-AVX2
> LDFLAGS_LOCAL   =       -ip -xHost -fp-model fast=2 -no-prec-div -no-prec-sqrt -ftz -align all -fno-alias -fno-common -xCORE-AVX2
175c142
< FCBASEOPTS_NO_G =       -ip -fp-model precise -w -ftz -align all -fno-alias $(FORMAT_FREE) $(BYTESWAPIO) #-xHost -fp-model fast=2 -no-heap-arrays -no-prec-div -no-prec-sqrt -fno-common
---
> FCBASEOPTS_NO_G =       -ip -fp-model precise -w -ftz -align all -fno-alias $(FORMAT_FREE) $(BYTESWAPIO) -xHost -fp-model fast=2 -no-heap-arrays -no-prec-div -no-prec-sqrt -fno-common -xCORE-AVX2
#+end_example

** Jobs with over 10000 mpi tasks
WRF produces ~rsl.out.XXXX~ and ~rsl.error.XXXX~ files for every mpi task, so to
support > 10000 mpi tasks, one must patch WRF to have a longer rsl file number
format otherwise one may encounter the following error:
#+begin_example
rsl_lite error ("buf_for_proc.c":119) Bad P argument to buffer_for_proc.  P = 18088. Has RSL_MESH been called?
#+end_example

* Results
** Code Snippets                                                   :noexport:
  :PROPERTIES:
  :header-args: :results silent :exports code
  :END:

#+begin_src ipython :session
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
%config InlineBackend.figure_format = 'svg'
# plt.style.use('dark_background')
#+end_src

#+begin_src ipython :session
def plot_scaling(cases, labels, title, shifts=None):
    if not shifts: shifts = [1] * len(cases)
    for case,label,shift in zip(cases,labels,shifts):
        try:
            runs = getFromDict(data, case)
            secs = np.array([d[2] for _,d in runs.items() if d[2] != '--'])
            cpus = np.array([d[7] for _,d in runs.items() if d[2] != '--'])
            gridpoints = e_we[case[0]] * e_sn[case[0]]
            plt.loglog(gridpoints/(cpus*shift), 1/secs, '+--', label=label[0])
        except: print("unable to plot case", case, "with label", label)
    plt.xlabel("Total grid points / core")
    plt.ylabel("Time steps / s")
    plt.legend(loc='upper center', bbox_to_anchor=(1.6, 1.0), ncol=1)
    #plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), ncol=2)
    plt.title(title)
    plt.axis('scaled')
    plt.grid(False)

def plot_bar(cases, nodes, labels, title):
    secs = []
    for case in cases:
        sec = getFromDict(data, case+[nodes])[2]
        secs.append(sec if isinstance(sec, float) else 0.0)

    secs = np.array(secs)
    labels = [l[0] for l in labels]
    y_pos = np.arange(len(cases))
    error = np.zeros(len(cases))

    fig, ax = plt.subplots()

    ax.barh(y_pos, 1/secs)
    ax.set_yticks(y_pos)
    ax.set_yticklabels(labels)
    ax.invert_yaxis()
    ax.set_xlabel('Time steps/s')
    ax.set_title(title)
#+end_src

#+begin_src ipython :session :var raw_data=data.org:data raw_gridpoints=data.org:gridpoints
from collections import defaultdict
from functools import reduce
from operator import getitem

def getFromDict(dataDict, mapList): return reduce(getitem, mapList, dataDict)
tree = lambda: defaultdict(tree)

data = tree()
for run in raw_data:
    keys = run[0].split('-')
    keys[0] = keys[0][2:]
    keys[-1] = int(keys[-1][1:-4])
    keys[-2] = int(keys[-2][1:])
    d = getFromDict(data,keys[:-1])
    d[keys[-1]] = run[1:]

e_we = {}
e_sn = {}
time_step = {}
for case in raw_gridpoints:
    e_we[case[0]] = case[1]
    e_sn[case[0]] = case[2]
    time_step[case[0]] = case[3]
#+end_src

** CONUS 12km :noexport:

#+begin_src sh :eval no
wrf_run_pbs_jobs \
    --wrfs ~/work/WRFs/WRFV3.8.1-gnu6.3.0-mpt2.18 \
    --cases ~/WRF_benchmarks/cases/conus12km \
    -n 1 2 4 8 16 32 64 128 256 -t 1
wrf_summarize_runs -r conus12km-WRFV3.8.1-gnu6.3.0-mpt2.18-T1-N* \
                   -o conus12km-WRFV3.8.1/gnu6.3.0/mpt2.18/T1 -t
#+end_src

#+name: conus12km-cases
| case      | WRF version | compiler    | mpi                | trial |
|-----------+-------------+-------------+--------------------+-------|
| conus12km | WRFV3.8.1   | gnu6.3.0    | mpt2.18            |     1 |
| conus12km | WRFV3.8.1   | gnu6.3.0    | mpt2.18            |     2 |
| conus12km | WRFV3.8.1   | gnu6.3.0    | mvapich2.2         |     1 |
| conus12km | WRFV3.8.1   | gnu6.3.0    | mvapich2.2         |     2 |
| conus12km | WRFV3.8.1   | gnu6.3.0    | mvapich2.2gnu7.1.0 |     1 |
| conus12km | WRFV3.8.1   | gnu6.3.0    | mvapich2.2gnu7.1.0 |     2 |
| conus12km | WRFV3.8.1   | intel18.0.1 | mpt2.18            |     1 |
| conus12km | WRFV3.8.1   | intel18.0.1 | mpt2.18            |     2 |

#+name: conus12km-labels
| label                          |
|--------------------------------|
| gnu6.3.0/mpt2.18/T1            |
| gnu6.3.0/mpt2.18/T2            |
| gnu6.3.0/mvapich2.2/T1         |
| gnu6.3.0/mvapich2.2/T2         |
| gnu6.3.0/mvapich2.2gnu7.1.0/T1 |
| gnu6.3.0/mvapich2.2gnu7.1.0/T2 |
| intel18.0.1/mpt2.18/T1         |
| intel18.0.1/mpt2.18/T2         |

** CONUS 12km
#+header: :var cases=conus12km-cases labels=conus12km-labels
#+header: :var title="Scaling results for CONUS 12km with WRFV3.8.1"
#+begin_src ipython :session :ipyfile ./imgs/conus12km.svg
plot_scaling(cases, labels, title)
#+end_src

#+RESULTS:
:RESULTS:
# Out[4]:
[[file:./imgs/conus12km.svg]]
:END:

** NEW CONUS 12km WRFV3.8.1 :noexport:

#+name: new_conus12km_3-cases
| case            | WRF version | compiler    | mpi        | trial |
|-----------------+-------------+-------------+------------+-------|
| new_conus12km_3 | WRFV3.8.1   | gnu6.3.0    | mpt2.18    |     1 |
| new_conus12km_3 | WRFV3.8.1   | gnu6.3.0    | mvapich2.2 |     1 |
| new_conus12km_3 | WRFV3.8.1   | intel18.0.1 | mpt2.18    |     1 |

#+name: new_conus12km_3-labels
| label               |
|---------------------|
| gnu6.3.0/mpt2.18    |
| gnu6.3.0/mvapich2.2 |
| intel18.0.1/mpt2.18 |
| intel18.0.1/mpt2.18 |

** NEW CONUS 12km WRFV3.8.1
#+header: :var cases=new_conus12km_3-cases labels=new_conus12km_3-labels
#+header: :var title="Scaling results for NEW CONUS 12km with WRFV3.8.1"
#+begin_src ipython :session :ipyfile ./imgs/new_conus12km_3.svg
plot_scaling(cases, labels, title)
#+end_src

#+RESULTS:
:RESULTS:
# Out[5]:
[[file:./imgs/new_conus12km_3.svg]]
:END:

** OLD CONUS 12km vs NEW CONUS 12km WRFV3.8.1 :noexport:

#+name: old-vs-new-conus12km-cases
| case            | WRF version | compiler    | mpi                | trial |
|-----------------+-------------+-------------+--------------------+-------|
| conus12km       | WRFV3.8.1   | gnu6.3.0    | mpt2.18            |     1 |
| conus12km       | WRFV3.8.1   | gnu6.3.0    | mpt2.18            |     2 |
| conus12km       | WRFV3.8.1   | gnu6.3.0    | mvapich2.2         |     1 |
| conus12km       | WRFV3.8.1   | gnu6.3.0    | mvapich2.2         |     2 |
| conus12km       | WRFV3.8.1   | gnu6.3.0    | mvapich2.2gnu7.1.0 |     1 |
| conus12km       | WRFV3.8.1   | gnu6.3.0    | mvapich2.2gnu7.1.0 |     2 |
| conus12km       | WRFV3.8.1   | intel18.0.1 | mpt2.18            |     1 |
| conus12km       | WRFV3.8.1   | intel18.0.1 | mpt2.18            |     2 |
| new_conus12km_3 | WRFV3.8.1   | gnu6.3.0    | mpt2.18            |     1 |
| new_conus12km_3 | WRFV3.8.1   | gnu6.3.0    | mvapich2.2         |     1 |
| new_conus12km_3 | WRFV3.8.1   | intel18.0.1 | mpt2.18            |     1 |

#+name: old-vs-new-conus12km-labels
| label                          |
|--------------------------------|
| gnu6.3.0/mpt2.18/T1            |
| gnu6.3.0/mpt2.18/T2            |
| gnu6.3.0/mvapich2.2/T1         |
| gnu6.3.0/mvapich2.2/T2         |
| gnu6.3.0/mvapich2.2gnu7.1.0/T1 |
| gnu6.3.0/mvapich2.2gnu7.1.0/T2 |
| intel18.0.1/mpt2.18/T1         |
| intel18.0.1/mpt2.18/T2         |
| gnu6.3.0/mpt2.18               |
| gnu6.3.0/mvapich2.2            |
| intel18.0.1/mpt2.18            |
| intel18.0.1/mpt2.18            |

** OLD CONUS 12km vs NEW CONUS 12km WRFV3.8.1
#+header: :var cases=old-vs-new-conus12km-cases labels=old-vs-new-conus12km-labels
#+header: :var title="Scaling results for OLD CONUS 12km vs NEW CONUS 12km with WRFV3.8.1"
#+begin_src ipython :session :ipyfile ./imgs/old_vs_new_conus12km_3.svg
plot_scaling(cases, labels, title)
#+end_src

#+RESULTS:
:RESULTS:
# Out[6]:
[[file:./imgs/old_vs_new_conus12km_3.svg]]
:END:

** NEW CONUS 12km :noexport:
#+begin_src sh :eval no
wrf_run_pbs_jobs \
    --wrfs \
    ~/work/WRFs/WRFV4.0-gnu8.1.0-mvapich2.2 \
    ~/work/WRFs/WRFV4.0-gnu8.1.0-fma-mvapich2.2 \
    ~/work/WRFs/WRFV4.0-gnu8.1.0-O3-mvapich2.2 \
    ~/work/WRFs/WRFV4.0-gnu8.1.0-O3-fma-mvapich2.2 \
    ~/work/WRFs/WRFV4.0-intel17.0.1-mvapich2.2 \
    ~/work/WRFs/WRFV4.0-intel18.0.1-mvapich2.2 \
    --cases ~/WRF_benchmarks/cases/new_conus12km \
    -n 1 2 4 8 16 -t 1

wrf_summarize_runs -r conus12km-WRFV3.8.1-gnu6.3.0-mpt2.18-T1-N* \
                   -o conus12km-WRFV3.8.1/gnu6.3.0/mpt2.18/T1 -t
#+end_src

#+name: new_conus12km-cases
| case          | WRF version | compiler              | mpi            | trial |
|---------------+-------------+-----------------------+----------------+-------|
| new_conus12km | WRFV4.0     | gnu6.3.0              | mpich3.2       |     1 |
| new_conus12km | WRFV4.0     | gnu6.3.0              | mpt2.18        |     1 |
| new_conus12km | WRFV4.0     | gnu8.1.0              | mpt2.18        |     1 |
| new_conus12km | WRFV4.0     | gnu8.1.0              | mvapich2.2     |     1 |
| new_conus12km | WRFV4.0     | gnu8.1.0              | openmpi3.1.0   |     1 |
| new_conus12km | WRFV4.0     | gnu8.1.0_O3           | mvapich2.2     |     1 |
| new_conus12km | WRFV4.0     | gnu8.1.0_fma          | mvapich2.2     |     1 |
| new_conus12km | WRFV4.0     | gnu8.1.0_O3_fma       | mvapich2.2     |     1 |
| new_conus12km | WRFV4.0     | gnu8.1.0_O3_native    | mpt2.18        |     1 |
| new_conus12km | WRFV4.0     | gnu8.1.0_Ofast        | mpt2.18        |     1 |
| new_conus12km | WRFV4.0     | gnu8.1.0_Ofast_native | mpt2.18        |     1 |
| new_conus12km | WRFV4.0     | intel18.0.1           | mpt2.18        |     1 |
| new_conus12km | WRFV4.0     | intel18.0.1           | openmpi3.1.0   |     1 |
| new_conus12km | WRFV4.0     | intel18.0.1           | impi2018.1.163 |     1 |
| new_conus12km | WRFV4.0     | intel18.0.1_host      | mpt2.18        |     1 |

#+name: new_conus12km-labels
| label                           |
|---------------------------------|
| gnu6.3.0 - mpich3.2             |
| gnu6.3.0 - mpt2.18              |
| gnu8.1.0 - mpt2.18              |
| gnu8.1.0 - mvapich2.2           |
| gnu8.1.0 - openmpi3.1.0         |
| gnu8.1.0_O3 - mvapich2.2        |
| gnu8.1.0_fma - mvapich2.2       |
| gnu8.1.0_O3_fma - mvapich2.2    |
| gnu8.1.0_O3_native - mpt2.18    |
| gnu8.1.0_Ofast - mpt2.18        |
| gnu8.1.0_Ofast_native - mpt2.18 |
| intel18.0.1 - mpt2.18           |
| intel18.0.1  - openmpi3.1.0     |
| intel18.0.1  - impi2018.1.163   |
| intel18.0.1_host - mpt2.18      |

** NEW CONUS 12km
#+header: :var cases=new_conus12km-cases labels=new_conus12km-labels
#+header: :var title="Scaling results for NEW CONUS 12km with WRFV4.0"
#+begin_src ipython :session :ipyfile ./imgs/new_conus12km.svg
plot_scaling(cases, labels, title)
#+end_src

#+RESULTS:
:RESULTS:
# Out[7]:
[[file:./imgs/new_conus12km.svg]]
:END:

** NEW CONUS 2.5km :noexport:

#+begin_src sh :eval no
wrf_run_pbs_jobs \
    --wrfs \
    ~/work/WRFs/WRFV4.0-gnu8.1.0-mvapich2.2 \
    ~/work/WRFs/WRFV4.0-gnu8.1.0-fma-mvapich2.2 \
    ~/work/WRFs/WRFV4.0-gnu8.1.0-O3-mvapich2.2 \
    ~/work/WRFs/WRFV4.0-gnu8.1.0-O3-fma-mvapich2.2 \
    ~/work/WRFs/WRFV4.0-intel17.0.1-mvapich2.2 \
    ~/work/WRFs/WRFV4.0-intel18.0.1-mvapich2.2 \
    --cases ~/WRF_benchmarks/cases/new_conus12km \
    -n 1 2 4 8 16 -t 1


wrf_run_pbs_jobs \
    --wrfs \
    ~/work/WRFs/WRFV4.0-intel17.0.1-mvapich2.2 \
    ~/work/WRFs/WRFV4.0-intel18.0.1-mvapich2.2 \
    ~/work/WRFs/WRFV4.0-gnu6.3.0-mvapich2.2 \
    ~/work/WRFs/WRFV4.0-gnu8.1.0-mvapich2.2 \
    ~/work/WRFs/WRFV4.0-gnu8.1.0-O3-fma-mvapich2.2 \
    ~/work/WRFs/WRFV4.0-gnu8.1.0-mpt2.18 \
    --cases \
    ~/WRF_benchmarks/cases/new_conus2.5km \
    ~/WRF_benchmarks/cases/maria3km \
    ~/WRF_benchmarks/cases/maria1km \
    -n 1 2 4 8 16 32 64 128 256 -t 2 -a '04:00:00'

wrf_summarize_runs -r conus12km-WRFV3.8.1-gnu6.3.0-mpt2.18-T1-N* \
                   -o conus12km-WRFV3.8.1/gnu6.3.0/mpt2.18/T1 -t
#+end_src

#+name: new_conus2.5km-cases
| case          | WRF version | compiler              | mpi            | trial |
|---------------+-------------+-----------------------+----------------+-------|
| new_conus12km | WRFV4.0     | gnu6.3.0              | mpich3.2       |     1 |
| new_conus12km | WRFV4.0     | gnu6.3.0              | mvapich2.2     |     1 |
| new_conus12km | WRFV4.0     | gnu8.1.0              | mpt2.18        |     1 |
| new_conus12km | WRFV4.0     | gnu8.1.0              | mvapich2.2     |     1 |
| new_conus12km | WRFV4.0     | gnu8.1.0              | openmpi3.1.0   |     1 |
| new_conus12km | WRFV4.0     | gnu8.1.0_O3           | mvapich2.2     |     1 |
| new_conus12km | WRFV4.0     | gnu8.1.0_O3_fma       | mvapich2.2     |     1 |
| new_conus12km | WRFV4.0     | gnu8.1.0_O3_native    | mpt2.18        |     1 |
| new_conus12km | WRFV4.0     | gnu8.1.0_Ofast        | mpt2.18        |     1 |
| new_conus12km | WRFV4.0     | gnu8.1.0_Ofast_native | mpt2.18        |     1 |
| new_conus12km | WRFV4.0     | intel18.0.1           | mpt2.18        |     1 |
| new_conus12km | WRFV4.0     | intel18.0.1           | openmpi3.1.0   |     1 |
| new_conus12km | WRFV4.0     | intel18.0.1           | impi2018.1.163 |     1 |
| new_conus12km | WRFV4.0     | intel18.0.1_host      | mpt2.18        |     1 |

#+name: new_conus2.5km-labels
| label                           |
|---------------------------------|
| gnu6.3.0 - mpich3.2             |
| gnu6.3.0 - mvapich2.2           |
| gnu8.1.0 - mpt2.18              |
| gnu8.1.0 - mvapich2.2           |
| gnu8.1.0 - openmpi3.1.0         |
| gnu8.1.0_O3 - mvapich2.2        |
| gnu8.1.0_O3_fma - mvapich2.2    |
| gnu8.1.0_O3_native - mpt2.18    |
| gnu8.1.0_Ofast - mpt2.18        |
| gnu8.1.0_Ofast_native - mpt2.18 |
| intel18.0.1 - mpt2.18           |
| intel18.0.1  - openmpi3.1.0     |
| intel18.0.1  - impi2018.1.163   |
| intel18.0.1_host - mpt2.18      |

** NEW CONUS 2.5km
#+header: :var cases=new_conus2.5km-cases labels=new_conus2.5km-labels
#+header: :var title="Scaling results for NEW CONUS 2.5km with WRFV4.0"
#+begin_src ipython :session :ipyfile ./imgs/new_conus2-5km.svg
plot_scaling(cases, labels, title)
#+end_src

#+RESULTS:
:RESULTS:
# Out[8]:
[[file:./imgs/new_conus2-5km.svg]]
:END:

** Maria 3km :noexport:

#+name: maria3km-cases
| case     | WRF version | compiler        | mpi        | trial |
|----------+-------------+-----------------+------------+-------|
| maria3km | WRFV4.0     | gnu6.3.0        | mvapich2.2 |     1 |
| maria3km | WRFV4.0     | gnu8.1.0        | mpt2.18    |     1 |
| maria3km | WRFV4.0     | gnu8.1.0        | mvapich2.2 |     1 |
| maria3km | WRFV4.0     | gnu8.1.0_O3_fma | mvapich2.2 |     1 |
| maria3km | WRFV4.0     | intel18.0.1     | mpt2.18    |     1 |

#+name: maria3km-labels
| label                      |
|----------------------------|
| gnu6.3.0/mvapich2.2        |
| gnu8.1.0/mpt2.18           |
| gnu8.1.0/mvapich2.2        |
| gnu8.1.0-O3-fma/mvapich2.2 |
| intel18.0.1/mpt2.18        |

** Maria 3km
#+header: :var cases=maria3km-cases labels=maria3km-labels
#+header: :var title="Scaling results for Maria 3km with WRFV4.0"
#+begin_src ipython :session :ipyfile ./imgs/maria3km.svg
plot_scaling(cases, labels, title)
#+end_src

#+RESULTS:
:RESULTS:
# Out[9]:
[[file:./imgs/maria3km.svg]]
:END:

** Cases table :noexport:
#+name: cases-cases
| case           | WRF version | compiler    | mpi         | trial |
|----------------+-------------+-------------+-------------+-------|
| new_conus12km  | WRFV4.0     | intel18.0.1 | mpt2.18     |     1 |
| new_conus2.5km | WRFV4.0     | intel18.0.1 | mpt2.18     |     1 |
| maria3km       | WRFV4.0     | intel18.0.1 | mpt2.18     |     1 |
| maria1km       | WRFV4.0     | intel18.0.1 | mpt2.18     |     1 |

#+name: cases-labels
| label            |
|------------------|
| new_conus12km    |
| new_conus2.5km   |
| maria3km         |
| maria1km         |

** Cases
#+header: :var cases=cases-cases labels=cases-labels
#+header: :var title="Scaling results for WRFV4.0/gnu8.1.0/mvapich2.2"
#+begin_src ipython :session :ipyfile ./imgs/cases.svg
plot_scaling(cases, labels, title)
#+end_src

#+attr_latex: :width 1.0\linewidth
#+RESULTS:
:RESULTS:
# Out[8]:
[[file:./imgs/cases.svg]]
:END:

** MPIs :noexport:
#+name: new_conus12km-cases-mpi
| case           | WRF version | compiler    | mpi            | trial |
|----------------+-------------+-------------+----------------+-------|
| new_conus12km  | WRFV4.0     | gnu6.3.0    | mpich3.2       |     1 |
| new_conus12km  | WRFV4.0     | gnu6.3.0    | mpt2.18        |     1 |
| new_conus12km  | WRFV4.0     | gnu8.1.0    | mpt2.18        |     1 |
| new_conus12km  | WRFV4.0     | gnu8.1.0    | mvapich2.2     |     1 |
| new_conus12km  | WRFV4.0     | gnu8.1.0    | openmpi3.1.0   |     1 |
| new_conus12km  | WRFV4.0     | intel18.0.1 | mpt2.18        |     1 |
| new_conus12km  | WRFV4.0     | intel18.0.1 | openmpi3.1.0   |     1 |
| new_conus12km  | WRFV4.0     | intel18.0.1 | impi2018.1.163 |     1 |

#+name: new_conus2.5km-cases-mpi
| case           | WRF version | compiler    | mpi            | trial |
|----------------+-------------+-------------+----------------+-------|
| new_conus2.5km | WRFV4.0     | gnu6.3.0    | mpich3.2       |     1 |
| new_conus2.5km | WRFV4.0     | gnu6.3.0    | mpt2.18        |     1 |
| new_conus2.5km | WRFV4.0     | gnu8.1.0    | mpt2.18        |     1 |
| new_conus2.5km | WRFV4.0     | gnu8.1.0    | mvapich2.2     |     1 |
| new_conus2.5km | WRFV4.0     | gnu8.1.0    | openmpi3.1.0   |     1 |
| new_conus2.5km | WRFV4.0     | intel18.0.1 | mpt2.18        |     1 |
| new_conus2.5km | WRFV4.0     | intel18.0.1 | openmpi3.1.0   |     1 |
| new_conus2.5km | WRFV4.0     | intel18.0.1 | impi2018.1.163 |     1 |

#+name: new_conus12km-labels-mpi
| label                         |
|-------------------------------|
| gnu6.3.0 - mpich3.2           |
| gnu6.3.0 - mpt2.18            |
| gnu8.1.0 - mpt2.18            |
| gnu8.1.0 - mvapich2.2         |
| gnu8.1.0 - openmpi3.1.0       |
| intel18.0.1 - mpt2.18         |
| intel18.0.1  - openmpi3.1.0   |
| intel18.0.1  - impi2018.1.163 |

#+name: new_conus2.5km-labels-mpi
| label                         |
|-------------------------------|
| gnu6.3.0 - mpich3.2           |
| gnu6.3.0 - mpt2.18            |
| gnu8.1.0 - mpt2.18            |
| gnu8.1.0 - mvapich2.2         |
| gnu8.1.0 - openmpi3.1.0       |
| intel18.0.1 - mpt2.18         |
| intel18.0.1  - openmpi3.1.0   |
| intel18.0.1  - impi2018.1.163 |

** MPIs
#+header: :var cases=new_conus12km-cases-mpi labels=new_conus12km-labels-mpi
#+header: :var title="Scaling results for NEW CONUS 12km with WRFV4.0"
#+begin_src ipython :session :ipyfile ./imgs/new_conus12km_mpi.svg
plot_scaling(cases, labels, title)
#+end_src

#+RESULTS:
:RESULTS:
# Out[11]:
[[file:./imgs/new_conus12km_mpi.svg]]
:END:

#+header: :var cases=new_conus2.5km-cases-mpi labels=new_conus2.5km-labels-mpi
#+header: :var title="Scaling results for NEW CONUS 2.5km with WRFV4.0"
#+begin_src ipython :session :ipyfile ./imgs/new_conus2-5km_mpi.svg
plot_scaling(cases, labels, title)
#+end_src

#+RESULTS:
:RESULTS:
# Out[12]:
[[file:./imgs/new_conus2-5km_mpi.svg]]
:END:

#+header: :var cases=new_conus12km-cases-mpi labels=new_conus12km-labels-mpi
#+header: :var title="Performance for normal number of nodes"
#+begin_src ipython :session :ipyfile ./imgs/new_conus12km_bar_normal.svg
plot_bar(cases, 16, labels, title)
#+end_src

#+RESULTS:
:RESULTS:
# Out[13]:
[[file:./imgs/new_conus12km_bar_normal.svg]]
:END:

#+header: :var cases=new_conus2.5km-cases-mpi labels=new_conus2.5km-labels-mpi
#+header: :var title="Performance for large number of nodes"
#+begin_src ipython :session :ipyfile ./imgs/new_conus12km_bar_large.svg
plot_bar(cases, 4, labels, title)
#+end_src

#+RESULTS:
:RESULTS:
# Out[14]:
[[file:./imgs/new_conus12km_bar_large.svg]]
:END:

** Compilers :noexport:
#+name: new_conus12km-cases-compiler
| case          | WRF version | compiler              | mpi        | trial |
|---------------+-------------+-----------------------+------------+-------|
| new_conus12km | WRFV4.0     | gnu8.1.0              | mpt2.18    |     1 |
| new_conus12km | WRFV4.0     | gnu8.1.0              | mvapich2.2 |     1 |
| new_conus12km | WRFV4.0     | gnu8.1.0_O3           | mvapich2.2 |     1 |
| new_conus12km | WRFV4.0     | gnu8.1.0_fma          | mvapich2.2 |     1 |
| new_conus12km | WRFV4.0     | gnu8.1.0_O3_fma       | mvapich2.2 |     1 |
| new_conus12km | WRFV4.0     | gnu8.1.0_O3_native    | mpt2.18    |     1 |
| new_conus12km | WRFV4.0     | gnu8.1.0_Ofast        | mpt2.18    |     1 |
| new_conus12km | WRFV4.0     | gnu8.1.0_Ofast_native | mpt2.18    |     1 |
| new_conus12km | WRFV4.0     | intel18.0.1           | mpt2.18    |     1 |
| new_conus12km | WRFV4.0     | intel18.0.1_host      | mpt2.18    |     1 |

#+name: new_conus12km-labels-compiler
| label                           |
|---------------------------------|
| gnu8.1.0 - mpt2.18              |
| gnu8.1.0 - mvapich2.2           |
| gnu8.1.0_O3 - mvapich2.2        |
| gnu8.1.0_fma - mvapich2.2       |
| gnu8.1.0_O3_fma - mvapich2.2    |
| gnu8.1.0_O3_native - mpt2.18    |
| gnu8.1.0_Ofast - mpt2.18        |
| gnu8.1.0_Ofast_native - mpt2.18 |
| intel18.0.1 - mpt2.18           |
| intel18.0.1_host - mpt2.18      |

#+name: new_conus2.5km-cases-compiler
| case           | WRF version | compiler              | mpi        | trial |
|----------------+-------------+-----------------------+------------+-------|
| new_conus2.5km | WRFV4.0     | gnu8.1.0              | mpt2.18    |     1 |
| new_conus2.5km | WRFV4.0     | gnu8.1.0              | mvapich2.2 |     1 |
| new_conus2.5km | WRFV4.0     | gnu8.1.0_O3           | mvapich2.2 |     1 |
| new_conus2.5km | WRFV4.0     | gnu8.1.0_fma          | mvapich2.2 |     1 |
| new_conus2.5km | WRFV4.0     | gnu8.1.0_O3_fma       | mvapich2.2 |     1 |
| new_conus2.5km | WRFV4.0     | gnu8.1.0_O3_native    | mpt2.18    |     1 |
| new_conus2.5km | WRFV4.0     | gnu8.1.0_Ofast        | mpt2.18    |     1 |
| new_conus2.5km | WRFV4.0     | gnu8.1.0_Ofast_native | mpt2.18    |     1 |
| new_conus2.5km | WRFV4.0     | intel18.0.1           | mpt2.18    |     1 |
| new_conus2.5km | WRFV4.0     | intel18.0.1_host      | mpt2.18    |     1 |

#+name: new_conus2.5km-labels-compiler
| label                           |
|---------------------------------|
| gnu8.1.0 - mpt2.18              |
| gnu8.1.0 - mvapich2.2           |
| gnu8.1.0_O3 - mvapich2.2        |
| gnu8.1.0_fma - mvapich2.2       |
| gnu8.1.0_O3_fma - mvapich2.2    |
| gnu8.1.0_O3_native - mpt2.18    |
| gnu8.1.0_Ofast - mpt2.18        |
| gnu8.1.0_Ofast_native - mpt2.18 |
| intel18.0.1 - mpt2.18           |
| intel18.0.1_host - mpt2.18      |

#+name: new_conus12km-cases-gnu
| case          | WRF version | compiler              | mpi          | trial |
|---------------+-------------+-----------------------+--------------+-------|
| new_conus12km | WRFV4.0     | gnu6.3.0              | mpich3.2     |     1 |
| new_conus12km | WRFV4.0     | gnu6.3.0              | mpt2.18      |     1 |
| new_conus12km | WRFV4.0     | gnu8.1.0              | mpt2.18      |     1 |
| new_conus12km | WRFV4.0     | gnu8.1.0              | mvapich2.2   |     1 |
| new_conus12km | WRFV4.0     | gnu8.1.0              | openmpi3.1.0 |     1 |
| new_conus12km | WRFV4.0     | gnu8.1.0_O3           | mvapich2.2   |     1 |
| new_conus12km | WRFV4.0     | gnu8.1.0_fma          | mvapich2.2   |     1 |
| new_conus12km | WRFV4.0     | gnu8.1.0_O3_fma       | mvapich2.2   |     1 |
| new_conus12km | WRFV4.0     | gnu8.1.0_O3_native    | mpt2.18      |     1 |
| new_conus12km | WRFV4.0     | gnu8.1.0_Ofast        | mpt2.18      |     1 |
| new_conus12km | WRFV4.0     | gnu8.1.0_Ofast_native | mpt2.18      |     1 |

#+name: new_conus12km-labels-gnu
| label                           |
|---------------------------------|
| gnu6.3.0 - mpich3.2             |
| gnu6.3.0 - mpt2.18              |
| gnu8.1.0 - mpt2.18              |
| gnu8.1.0 - mvapich2.2           |
| gnu8.1.0 - openmpi3.1.0         |
| gnu8.1.0_O3 - mvapich2.2        |
| gnu8.1.0_fma - mvapich2.2       |
| gnu8.1.0_O3_fma - mvapich2.2    |
| gnu8.1.0_O3_native - mpt2.18    |
| gnu8.1.0_Ofast - mpt2.18        |
| gnu8.1.0_Ofast_native - mpt2.18 |

#+name: new_conus12km-cases-intel
| case          | WRF version | compiler           | mpi            | trial |
|---------------+-------------+--------------------+----------------+-------|
| new_conus12km | WRFV4.0     | intel18.0.1        | mpt2.18        |     1 |
| new_conus12km | WRFV4.0     | intel18.0.1        | mpt2.18        |     2 |
| new_conus12km | WRFV4.0     | intel18.0.1_host   | mpt2.18        |     1 |
| new_conus12km | WRFV4.0     | intel18.0.1        | openmpi3.1.0   |     1 |
| new_conus12km | WRFV4.0     | intel18.0.1        | impi2018.1.163 |     1 |

#+name: new_conus12km-labels-intel
| label                         |
|-------------------------------|
| intel18.0.1 - mpt2.18         |
| intel18.0.1 - mpt2.18         |
| intel18.0.1_host - mpt2.18    |
| intel18.0.1  - openmpi3.1.0   |
| intel18.0.1  - impi2018.1.163 |

** Compilers
#+header: :var cases=new_conus12km-cases-compiler labels=new_conus12km-labels-compiler
#+header: :var title="Scaling results for NEW CONUS 12km with WRFV4.0"
#+begin_src ipython :session :ipyfile ./imgs/new_conus12km_compiler.svg
plot_scaling(cases, labels, title)
#+end_src

#+RESULTS:
:RESULTS:
# Out[15]:
[[file:./imgs/new_conus12km_compiler.svg]]
:END:

#+header: :var cases=new_conus2.5km-cases-compiler labels=new_conus2.5km-labels-compiler
#+header: :var title="Scaling results for NEW CONUS 2.5km with WRFV4.0"
#+begin_src ipython :session :ipyfile ./imgs/new_conus2-5km_compiler.svg
plot_scaling(cases, labels, title)
#+end_src

#+RESULTS:
:RESULTS:
# Out[16]:
[[file:./imgs/new_conus2-5km_compiler.svg]]
:END:

#+header: :var cases=new_conus12km-cases-gnu labels=new_conus12km-labels-gnu
#+header: :var title="Scaling results for NEW CONUS 12km with WRFV4.0"
#+begin_src ipython :session :ipyfile ./imgs/new_conus12km_gnu.svg
plot_scaling(cases, labels, title)
#+end_src

#+RESULTS:
:RESULTS:
# Out[17]:
[[file:./imgs/new_conus12km_gnu.svg]]
:END:

#+header: :var cases=new_conus12km-cases-intel labels=new_conus12km-labels-intel
#+header: :var title="Scaling results for NEW CONUS 12km with WRFV4.0"
#+begin_src ipython :session :ipyfile ./imgs/new_conus12km_intel.svg
plot_scaling(cases, labels, title)
#+end_src

#+RESULTS:
:RESULTS:
# Out[18]:
[[file:./imgs/new_conus12km_intel.svg]]
:END:

** Hybrid :noexport:
#+name: cases-omp
| case           | WRF version | compiler    | mpi                | trial |
|----------------+-------------+-------------+--------------------+-------|
| new_conus12km  | WRFV4.0     | intel18.0.1 | mpt2.18            |     1 |
| new_conus12km  | WRFV4.0     | intel18.0.1 | mpt2.18_omp_m2_o18 |     1 |
| new_conus12km  | WRFV4.0     | intel18.0.1 | mpt2.18_omp_m3_o12 |     1 |
| new_conus12km  | WRFV4.0     | intel18.0.1 | mpt2.18_omp_m4_o9  |     1 |
| new_conus12km  | WRFV4.0     | intel18.0.1 | mpt2.18_omp_m6_o6  |     1 |
| new_conus2.5km | WRFV4.0     | intel18.0.1 | mpt2.18            |     1 |
| new_conus2.5km | WRFV4.0     | intel18.0.1 | mpt2.18_omp_m2_o18 |     1 |
| new_conus2.5km | WRFV4.0     | intel18.0.1 | mpt2.18_omp_m3_o12 |     1 |
| new_conus2.5km | WRFV4.0     | intel18.0.1 | mpt2.18_omp_m4_o9  |     1 |
| new_conus2.5km | WRFV4.0     | intel18.0.1 | mpt2.18_omp_m6_o6  |     1 |


#+name: labels-omp
| label                          |
|--------------------------------|
| new_conus12km                  |
| new_conus12km - 2 MPI, 18 OMP  |
| new_conus12km - 3 MPI, 12 OMP  |
| new_conus12km - 4 MPI, 9 OMP   |
| new_conus12km - 6 MPI, 6 OMP   |
| new_conus2.5km                 |
| new_conus2.5km - 2 MPI, 18 OMP |
| new_conus2.5km - 3 MPI, 12 OMP |
| new_conus2.5km - 4 MPI, 9 OMP  |
| new_conus2.5km - 6 MPI, 6 OMP  |

** Hybrid
#+header: :var cases=cases-omp labels=labels-omp
#+header: :var title="Scaling results for Hybrid WRFV4.0"
#+begin_src ipython :session :ipyfile ./imgs/hybrid.svg
plot_scaling(cases, labels, title, shifts=[1, 18, 12, 9, 6,  1, 18, 12, 9, 6])
#+end_src

#+RESULTS:
:RESULTS:
# Out[14]:
[[file:./imgs/hybrid.svg]]
:END:

#+header: :var cases=cases-omp labels=labels-omp
#+header: :var title="Performance for normal number of nodes"
#+begin_src ipython :session :ipyfile ./imgs/hybrid_bar_normal.svg
cases=cases[:5]; labels=labels[:5]
plot_bar(cases, 16, labels, title)
#+end_src

#+RESULTS:
:RESULTS:
# Out[21]:
[[file:./imgs/hybrid_bar_normal.svg]]
:END:

#+header: :var cases=cases-omp labels=labels-omp
#+header: :var title="Performance for large number of nodes"
#+begin_src ipython :session :ipyfile ./imgs/hybrid_bar_large.svg
cases=cases[5:]; labels=labels[5:]
plot_bar(cases, 8, labels, title)
#+end_src

#+RESULTS:
:RESULTS:
# Out[19]:
[[file:./imgs/hybrid_bar_large.svg]]
:END:

* WRF scaling and timing on Cheyenne[fn:: [[https://www2.cisl.ucar.edu/software/community-models/wrf-scaling-and-timing][A similar study for Yellowstone]]]
- "Is it possible to solve a problem with such-and-such resolution in a timely
  manner?"
- "If I use more cores I will have the results more quickly, but with this
  resolution will my run be in the efficient strong-scaling regime, an
  intermediate one, or in the very inefficient one dominated by I/O and
  initialization instead of computing?"

Numbers from the figures below can help you develop some back-of-the-envelope
estimates of what will happen if you increase or decrease the core counts of
your runs, so that you can find one that is optimal for you both in terms of
time-to-solution and in terms of your allocation.

--------------------------------------------------------------------------------

** Scaling Results

Figure [[wrf_scaling]] shows scaling results from Hurricane Maria simulations at 3km
and 1km resolutions along with CONUS simulations at 12km and 2.5km resolutions.
When expressed this way, all the cases scale similarly. Note that both axes are
logarithmic, so a small distance between points corresponds to a large
difference in values.

#+name: wrf_scaling
#+caption: WRF scaling
[[file:./imgs/cases.svg]]

As you can see, there are three regimes:

- large number of grid points per core - *Total grid points / core > 10^5*
  (small core count)
- intermediate number of grid points per core - *10^4 < Total grid points / core
  < 10^5* (intermediate core count)
- small number of grid points per core - *Total grid points / core < 10^4*
  (large core count)

For a small number of cores, the WRF computation kernel is in a strong scaling
regime. Increasing the core count will make the simulation go faster while it
consumes approximately the same amount of core-hours (ignoring time spent in
initialization and I/O). Time-to-solution will also depend on the wait in queue,
which may be larger for larger jobs.

For an intermediate number of cores, WRF scaling increasingly departs from
linear strong scaling. Running the same simulation on larger core counts will
require more core-hours even though it will still run faster (again, ignoring
time spent in initialization, I/O, and wait in queue).

We do not recommend running WRF on extremely large core counts, because in this
regime the speed benefits diminish, the time will be dominated by initialization
and I/O (as well as wait in queue), and there will be larger core-hours charges
for solving the same problem.

** Run time code :noexport:
#+begin_src ipython :session
def plot_maria1km_time():
    maria_case = ['maria1km', 'WRFV4.0', 'intel18.0.1', 'mpt2.18',  1 ]
    runs = getFromDict(data, maria_case)
    cpus = np.array([d[7] for _,d in runs.items() if d[0] != '--'])
    comp = np.array([d[0] for _,d in runs.items() if d[0] != '--'])
    io = np.array([d[3] for _,d in runs.items() if d[0] != '--'])
    gridpoints = e_we[case[0]] * e_sn[case[0]]
    plt.plot(cpus, comp, 'o', label='computation')
    plt.plot(cpus, io, '+', label='io time')
    plt.xlabel("cores")
    plt.ylabel("wall clock time (seconds)")
    plt.legend(loc='best')
    #plt.title(title)
    #plt.axis('scaled')
    plt.grid(False)
#+end_src

#+RESULTS:
:RESULTS:
# Out[25]:
:END:

** Run time results

Figure [[wrf_runtime]] below shows the total run time for WRF jobs using increasing
numbers of cores. Initialization time, computation time, and writing time also
are shown for runs using up to 8,192 cores. 

These results are based on simulations of Hurricane Maria (2017) at 1km
resolution.

As illustrated, initialization and writing output times may be more expensive
than computing time for larger core counts. Times shown are for a and single
output file of the Maria 1km case, which used a domain of about 372 million grid
points. If you have more restarts and output files, your numbers will be
different but the trend will be similar.

#+begin_src ipython :session :ipyfile ./imgs/maria1km_runtime.svg
plot_maria1km_time()
#+end_src

#+name: wrf_runtime
#+caption: WRF Run Timing
#+RESULTS:
:RESULTS:
# Out[26]:
[[file:./imgs/maria1km_runtime.svg]]
:END:

* Summary
** Conclusions
- openMPI, MPT, and MVAPICH show similar runtime performance.
- intel consistently faster than gnu
