# -*- org-html-htmlize-output-type: css; -*-
#+title: Weather Research and Forecast (WRF) Scaling and Performance Assessment
#+subtitle: NCAR SIParCS Program
#+author: Akira Kyle
#+email: akyle@cmu.edu
#+property: header-args :results raw drawer :exports results :eval never-export
#+options: toc:t num:nil email:t author:t date:nil
#+setupfile: theme-readtheorg-local.setup
# #+html_head: <style type="text/css">body{ max-width:50em; margin-left:auto; margin-right:auto; }</style>

#+begin_warning
This is still very much a work in progress!
#+end_warning

* Abstract
  :PROPERTIES:
  :CUSTOM_ID: abstract
  :END:

The Weather Research and Forecast (WRF) model is a parallel mesoscale numerical
weather forecasting application used in both operational and research
environments. The performance of WRF on NCAR's Cheyenne supercomputer was
investigated, focusing primarily on run time and compile time settings. The
latest Intel (18.0.1) and Gnu (8.1.0) compilers were compared with different
compilation flags. We found the Intel compiler to be consistently faster than
Gnu at various optimization levels. Various MPI libraries were tested, including
MPT, MVAPICH, Intel MPI, MPICH. We found that openMPI, MPT, and MVAPICH show
similar runtime performance while the performance of MPICH was poor and Intel
MPI's performance scaled poorly to large node counts. Several benchmark cases
were developed for the latest version of WRF (4.0) at different resolutions and
utilizing the CONUS and tropical physics suites. The scaling results of these
benchmark cases were used to give users of Cheyenne recommendations on how to
run WRF in a timely and efficient manner. Additionally we compared the scaling
of WRF's initialization time, I/O time, and computation time and found better
scaling to very large node counts than with previous WRF versions on NCAR's
previous supercomputer, Yellowstone.

* Background
  :PROPERTIES:
  :CUSTOM_ID: background
  :END:
** The Weather Research and Forecast Model
   :PROPERTIES:
   :CUSTOM_ID: weather_research_forcast_model
   :END:

The Weather Research and Forecast (WRF) model is a parallel mesoscale numerical
weather forecasting application used in both operational and research
environments. WRF is among the more commonly run codes by atmospheric scientists
on NCAR's Cheyenne supercomputer. Hence it is very important for WRF's users to
know how to obtain the best performance of WRF on Cheyenne, especially as users
scale their runs to larger core counts.

** WRF System Flowchart
   :PROPERTIES:
   :CUSTOM_ID: wrf_flowchart
   :END:
#+ATTR_LATEX: :width 0.9\linewidth
[[./imgs/WRF_flow_chart-ARW_v4.png]]

** Cheyenne
   :PROPERTIES:
   :CUSTOM_ID: cheyenne_specs
   :END:

- 4,032 computation nodes
 	- Dual-socket nodes, 18 cores per socket
    - 145,152 total processor cores
  - 2.3-GHz Intel Xeon E5-2697V4 (Broadwell) processors
    - 16 flops per clock
  - 5.34 peak petaflops
- 313 TB total system memory
  - 64 GB/node on 3,168 nodes, DDR4-2400
  - 128 GB/node on 864 nodes, DDR4-2400
- Mellanox EDR InfiniBand high-speed interconnect
  - Partial 9D Enhanced Hypercube single-plane interconnect topology
  - Bandwidth: 25 GBps bidirectional per link
  - Latency: MPI ping-pong < 1 µs; hardware link 130 ns

#+begin_note
[[https://www2.cisl.ucar.edu/resources/computational-systems/cheyenne][More Details on the Cheyenne supercomputer can be found here]]
#+end_note

* Intro
  :PROPERTIES:
  :CUSTOM_ID: intro
  :END:
** Test cases
   :PROPERTIES:
   :CUSTOM_ID: test_cases
   :END:
- ~conus12km~ and ~conus2.5km~
  - Official CONUS benchmarks from http://www2.mmm.ucar.edu/wrf/WG2/benchv3/
  - Benchmarks no longer maintained
  - Only works on WRFV3.8.1 or earlier
  - ~wrfbdy~ generated using WRFV2.2 and ~wrfrst~ generated using WRFV3.2beta
    provided
- ~katrina1km~ ~katrina3km~
  - cases based upon Katrina single domain case in WRF-ARW online tutorial
    (http://www2.mmm.ucar.edu/wrf/OnLineTutorial/CASES/SingleDomain/index.html)
  - Same domain and similar physics but higher resolution
  - cfl errors encountered often, especially with ~1km~ resolution
    - Mostly likely due to high vertical winds over mountain ranges in Mexico
- ~new_conus12km~ and ~new_conus2.5km~
  - Based on official CONUS benchmarks
  - 12km and 2.5km vertical and horizontal resolution respectively
  - Use CONUS Physics suite introduced in WRFV3.9 instead of physics used in
    above official CONUS benchmarks
  - ~cu_physics = 0~ for ~new_conus2.5~
  - NCEP GFS 0.25 deg input data for 2018-06-17
  - 6 hour run, model output at 0, 3, 6hrs, restart file at 6hrs
- ~maria3km~ and ~maria1km~
  - 3km and 1km vertical and horizontal resolution respectively
  - TROPICAL Physics suite with ~cu_physics~ disabled and ~sf_sfclay_physics = 1~

| case             | e_we | e_sn | total gridpoints | timestep | run hours |
|------------------+------+------+------------------+----------+-----------|
| ~conus12km~      |  425 |  300 | 127,500          |       72 |         3 |
| ~conus2.5km~     | 1901 | 1301 | 2,473,201        |       15 |         6 |
| ~new_conus12km~  |  425 |  300 | 127,500          |       72 |         6 |
| ~new_conus2.5km~ | 1901 | 1301 | 2,473,201        |       15 |         6 |
| ~maria1km~       | 3665 | 2894 | 10,606,510       |        3 |         1 |
| ~maria3km~       | 1396 | 1384 | 1,932,064        |        9 |         3 |

** Compilers
   :PROPERTIES:
   :CUSTOM_ID: compilers
   :END:

- GNU Compiler Collection (GCC) versions 6.3.0, 8.1.0
  - WRF compiles with ~-O2~ by default
  - ~-O3~ : enables all ~-O2~ optimization along with optimizations such as
    function inlining and loop vectorization
  - ~-Ofast~ : enables all ~-O3~ optimizations along with disregarding strict
    standards compliance (such is for floating point operations)
  - ~-mfma~ : enables Fused Multiply-Add instruction set
  - ~-march=native~ : enables target instruction set to be everything
    supported by the compiling machine
- Intel Compiler versions 17.0.1, 18.0.1
  - WRF compiles with ~-O3~ by default
  - ~-Xhost~ : similar to GNU's ~-march=native~
  - ~-fp-model fast=2~ : similar to GNU's ~-Ofast~ optimization

** Message Passing Interface Libraries
   :PROPERTIES:
   :CUSTOM_ID: mpi_libraries
   :END:
- SGI's MPT version 2.18 (Default MPI for Cheyenne)
- Ohio State University's MVAPICH version 2.2
- OpenMPI version 3.1.0
- Intel MPI version 2018.1.163
- MPICH version 3.2

* Observations
  :PROPERTIES:
  :CUSTOM_ID: observations
  :END:
** Compilation issue with gnu > 6.3.0 and WRF < V4.0
   :PROPERTIES:
   :CUSTOM_ID: gnu_compilation_issue
   :END:
 This likely require a source patch to fix, not an environment or compiler issue
#+begin_example
module_cu_g3.f90:3132:41:

                    call random_seed (PUT=seed)
                                         1
Error: Size of ‘put’ argument of ‘random_seed’ intrinsic at (1) too small (12/33)
#+end_example

** ~new_conus12km~ on WRFV4.0 exceeds minimum patch size on >= 32 nodes (36 cpus per node)
   :PROPERTIES:
   :CUSTOM_ID: exceeding_min_patch_size
   :END:
#+begin_example
taskid: 0 hostname: r14i1n17
 module_io_quilt_old.F        2931 F
Quilting with   1 groups of   0 I/O tasks.
 Ntasks in X           32 , ntasks in Y           36
*************************************
Configuring physics suite 'conus'

         mp_physics:      8
         cu_physics:      6
      ra_lw_physics:      4
      ra_sw_physics:      4
     bl_pbl_physics:      2
  sf_sfclay_physics:      2
 sf_surface_physics:      2
*************************************
   For domain            1 , the domain size is too small for this many processors, or the decomposition aspect ratio is poor.
   Minimum decomposed computational patch size, either x-dir or y-dir, is 10 grid cells.
  e_we =   425, nproc_x =   32, with cell width in x-direction =   13
  e_sn =   300, nproc_y =   36, with cell width in y-direction =    8
  --- ERROR: Reduce the MPI rank count, or redistribute the tasks.
-------------- FATAL CALLED ---------------
FATAL CALLED FROM FILE:  <stdin>  LINE:    1726
NOTE:       1 namelist settings are wrong. Please check and reset these options
-------------------------------------------
#+end_example

#+begin_note
This is a change in WRFV4.0 from previous versions. From
http://www2.mmm.ucar.edu/wrf/users/wrfv4.0/updates-4.0.html:
-  "Determine the resultant x- and y-direction computational patch sizes based
  on the full domain size and the number of MPI ranks used in each direction. If
  the patch size < 10 grid cells in either direction, the model will stop."
#+end_note

** Not enough memory for small node count on large cases
   :PROPERTIES:
   :CUSTOM_ID: out_of_mem
   :END:
When run on cheyenne's 64 GB memory nodes
(https://www2.cisl.ucar.edu/resources/computational-systems/cheyenne), WRFV4.0
usually see jobs SIGKILLed at various points during startup for
- ~new_conus2.5~ on <= 2 nodes
- ~maria3km~ on 1 node
- ~maria1km~ on <= 8 nodes

** Intel takes ~2x as long to compile wrf then gnu
   :PROPERTIES:
   :CUSTOM_ID: intel_compilation_time
   :END:
** WRF compilation option 66/67
   :PROPERTIES:
   :CUSTOM_ID: wrf_compilation_opt_66_67
   :END:
Relevant diff of ~configure.wrf~:
#+begin_example
5c5
< # Compiler choice: 15
---
> # Compiler choice: 66
161,163c128,130
< ARCH_LOCAL      =       -DNONSTANDARD_SYSTEM_FUNC  -DWRF_USE_CLM $(NETCDF4_IO_OPTS)
< CFLAGS_LOCAL    =       -w -O3 -ip #-xHost -fp-model fast=2 -no-prec-div -no-prec-sqrt -ftz -no-multibyte-chars
< LDFLAGS_LOCAL   =       -ip #-xHost -fp-model fast=2 -no-prec-div -no-prec-sqrt -ftz -align all -fno-alias -fno-common
---
> ARCH_LOCAL      =       -DNONSTANDARD_SYSTEM_FUNC -DWRF_USE_CLM $(NETCDF4_IO_OPTS)
> CFLAGS_LOCAL    =       -w -O3 -ip -xHost -fp-model fast=2 -no-prec-div -no-prec-sqrt -ftz -no-multibyte-chars -xCORE-AVX2
> LDFLAGS_LOCAL   =       -ip -xHost -fp-model fast=2 -no-prec-div -no-prec-sqrt -ftz -align all -fno-alias -fno-common -xCORE-AVX2
175c142
< FCBASEOPTS_NO_G =       -ip -fp-model precise -w -ftz -align all -fno-alias $(FORMAT_FREE) $(BYTESWAPIO) #-xHost -fp-model fast=2 -no-heap-arrays -no-prec-div -no-prec-sqrt -fno-common
---
> FCBASEOPTS_NO_G =       -ip -fp-model precise -w -ftz -align all -fno-alias $(FORMAT_FREE) $(BYTESWAPIO) -xHost -fp-model fast=2 -no-heap-arrays -no-prec-div -no-prec-sqrt -fno-common -xCORE-AVX2
#+end_example

** Jobs with over 10000 mpi tasks
   :PROPERTIES:
   :CUSTOM_ID: over_10000_mpi_tasks
   :END:
WRF produces ~rsl.out.XXXX~ and ~rsl.error.XXXX~ files for every mpi task, so to
support > 10000 mpi tasks, one must patch WRF to have a longer rsl file number
format otherwise one may encounter the following error:
#+begin_example
rsl_lite error ("buf_for_proc.c":119) Bad P argument to buffer_for_proc.  P = 18088. Has RSL_MESH been called?
#+end_example

A patch for WRFV4.0 can be found at ~WRFs/WRFV4.0-rsl-8digit.patch~

* Results
  :PROPERTIES:
  :CUSTOM_ID: results
  :END:

#+begin_note
All the raw data used to generate the plots seen here can be [[https://github.com/akirakyle/WRF_benchmarks/blob/master/docs/data.org][found here]].
#+end_note

** Code Snippets                                                   :noexport:
  :PROPERTIES:
  :header-args: :results silent :exports code
  :END:

#+begin_src ipython :session
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
%config InlineBackend.figure_format = 'svg'
# plt.style.use('dark_background')
#+end_src

#+begin_src ipython :session
def plot_scaling(cases, labels, title, shifts=None, legend_outside=True,
                 line=None, fmts=None):
    if not shifts: shifts = [1] * len(cases)
    if not fmts: fmts = ['o--'] * len(cases)
    plt.figure(figsize=(3.5,3.5))
    if line: plt.loglog([150,3500000],[25,0.002], 'b-')#, linewidth=2.0)
    for case,label,shift,fmt in zip(cases,labels,shifts,fmts):
        gridpoints = e_we[case[0]] * e_sn[case[0]]
        secs = []; cpus = []
        for trial in case[4].split(','):
            run = getFromDict(data, case[:4]+[trial]+case[5:])
            secs += [d[2] for _,d in run.items()]
            cpus += [d[7]*d[8] for _,d in run.items()]
        secs = np.array(secs); cpus = np.array(cpus)
        p = secs.argsort()
        plt.loglog(gridpoints/(cpus[p]*shift), 1/secs[p], fmt, label=label[0])
    plt.xlabel("Total grid points / core", fontweight='bold')
    plt.ylabel("Time steps / s", fontweight='bold')
    if legend_outside: plt.legend(loc='upper center', bbox_to_anchor=(1.6, 1.0), ncol=1)
    else: plt.legend(loc='best', prop={'size': 8})
    #plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), ncol=2)
    plt.title(title)
    # plt.axis('scaled')
    plt.grid(False)
#+end_src

#+begin_src ipython :session
def plot_bar(cases, nodes, labels, title):
    secs = []
    for case in cases:
        sec = getFromDict(data, case+[nodes])[2]
        secs.append(sec if isinstance(sec, float) else 0.0)

    secs = np.array(secs)
    labels = [l[0] for l in labels]
    y_pos = np.arange(len(cases))
    error = np.zeros(len(cases))

    fig, ax = plt.subplots()

    ax.barh(y_pos, 1/secs)
    ax.set_yticks(y_pos)
    ax.set_yticklabels(labels)
    ax.invert_yaxis()
    ax.set_xlabel('Time steps/s')
    plt.legend(loc='best', prop={'size': 8})
    ax.set_title(title)
#+end_src

#+begin_src ipython :session :var raw_gridpoints=data.org:gridpoints raw_data=data.org:data raw_dixit_data=data.org:dixit-data
from collections import defaultdict
from functools import reduce
from operator import getitem

e_we = {}
e_sn = {}
time_step = {}
for case in raw_gridpoints:
    e_we[case[0]] = case[1]
    e_sn[case[0]] = case[2]
    time_step[case[0]] = case[3]

def getFromDict(dataDict, mapList): return reduce(getitem, mapList, dataDict)
tree = lambda: defaultdict(tree)

data = tree()
for run in raw_data:
    keys = run[0].split('-')
    d = getFromDict(data,keys[:-1])
    d[keys[-1]] = run[1:]

dixit_data = tree()
for run in raw_dixit_data:
    keys = run[0].split('-')
    d = getFromDict(dixit_data,keys[:-1])
    d[keys[-1]] = run[1:]

data = {**data, **dixit_data}
#+end_src

** CONUS 12km :noexport:

#+name: conus12km-cases
| case      | WRF version | compiler    | mpi                | trial |
|-----------+-------------+-------------+--------------------+-------|
| conus12km | WRFV3.8.1   | gnu6.3.0    | mpt2.18            | T1    |
| conus12km | WRFV3.8.1   | gnu6.3.0    | mpt2.18            | T2    |
| conus12km | WRFV3.8.1   | gnu6.3.0    | mvapich2.2         | T1    |
| conus12km | WRFV3.8.1   | gnu6.3.0    | mvapich2.2         | T2    |
| conus12km | WRFV3.8.1   | gnu6.3.0    | mvapich2.2gnu7.1.0 | T1    |
| conus12km | WRFV3.8.1   | gnu6.3.0    | mvapich2.2gnu7.1.0 | T2    |
| conus12km | WRFV3.8.1   | intel18.0.1 | mpt2.18            | T1    |
| conus12km | WRFV3.8.1   | intel18.0.1 | mpt2.18            | T2    |

#+name: conus12km-labels
| label                          |
|--------------------------------|
| gnu6.3.0-mpt2.18-T1            |
| gnu6.3.0-mpt2.18-T2            |
| gnu6.3.0-mvapich2.2-T1         |
| gnu6.3.0-mvapich2.2-T2         |
| gnu6.3.0-mvapich2.2gnu7.1.0-T1 |
| gnu6.3.0-mvapich2.2gnu7.1.0-T2 |
| intel18.0.1-mpt2.18-T1         |
| intel18.0.1-mpt2.18-T2         |

** CONUS 12km
   :PROPERTIES:
   :CUSTOM_ID: conus12km
   :END:
#+header: :var cases=conus12km-cases labels=conus12km-labels
#+header: :var title="Scaling results for CONUS 12km with WRFV3.8.1"
#+begin_src ipython :session :ipyfile ./figs/conus12km.svg
plot_scaling(cases, labels, title)
#+end_src

#+RESULTS:
:RESULTS:
# Out[54]:
[[file:./figs/conus12km.svg]]
:END:

** NEW CONUS 12km WRFV3.8.1 :noexport:

#+name: new_conus12km_3-cases
| case            | WRF version | compiler    | mpi        | trial |
|-----------------+-------------+-------------+------------+-------|
| new_conus12km_3 | WRFV3.8.1   | gnu6.3.0    | mpt2.18    | T1    |
| new_conus12km_3 | WRFV3.8.1   | gnu6.3.0    | mvapich2.2 | T1    |
| new_conus12km_3 | WRFV3.8.1   | intel18.0.1 | mpt2.18    | T1    |

#+name: new_conus12km_3-labels
| label               |
|---------------------|
| gnu6.3.0-mpt2.18    |
| gnu6.3.0-mvapich2.2 |
| intel18.0.1-mpt2.18 |
| intel18.0.1-mpt2.18 |

** NEW CONUS 12km WRFV3.8.1
   :PROPERTIES:
   :CUSTOM_ID: new_conus12km_wrfv3_8_1
   :END:
#+header: :var cases=new_conus12km_3-cases labels=new_conus12km_3-labels
#+header: :var title="Scaling results for NEW CONUS 12km with WRFV3.8.1"
#+begin_src ipython :session :ipyfile ./figs/new_conus12km_3.svg
plot_scaling(cases, labels, title)
#+end_src

#+RESULTS:
:RESULTS:
# Out[57]:
[[file:./figs/new_conus12km_3.svg]]
:END:

** OLD CONUS 12km vs NEW CONUS 12km WRFV3.8.1 :noexport:

#+name: old-vs-new-conus12km-cases
| case            | WRF version | compiler    | mpi                | trial |
|-----------------+-------------+-------------+--------------------+-------|
| conus12km       | WRFV3.8.1   | gnu6.3.0    | mpt2.18            | T1    |
| conus12km       | WRFV3.8.1   | gnu6.3.0    | mpt2.18            | T2    |
| conus12km       | WRFV3.8.1   | gnu6.3.0    | mvapich2.2         | T1    |
| conus12km       | WRFV3.8.1   | gnu6.3.0    | mvapich2.2         | T2    |
| conus12km       | WRFV3.8.1   | gnu6.3.0    | mvapich2.2gnu7.1.0 | T1    |
| conus12km       | WRFV3.8.1   | gnu6.3.0    | mvapich2.2gnu7.1.0 | T2    |
| conus12km       | WRFV3.8.1   | intel18.0.1 | mpt2.18            | T1    |
| conus12km       | WRFV3.8.1   | intel18.0.1 | mpt2.18            | T2    |
| new_conus12km_3 | WRFV3.8.1   | gnu6.3.0    | mpt2.18            | T1    |
| new_conus12km_3 | WRFV3.8.1   | gnu6.3.0    | mvapich2.2         | T1    |
| new_conus12km_3 | WRFV3.8.1   | intel18.0.1 | mpt2.18            | T1    |

#+name: old-vs-new-conus12km-labels
| label                          |
|--------------------------------|
| gnu6.3.0-mpt2.18-T1            |
| gnu6.3.0-mpt2.18-T2            |
| gnu6.3.0-mvapich2.2-T1         |
| gnu6.3.0-mvapich2.2-T2         |
| gnu6.3.0-mvapich2.2gnu7.1.0-T1 |
| gnu6.3.0-mvapich2.2gnu7.1.0-T2 |
| intel18.0.1-mpt2.18-T1         |
| intel18.0.1-mpt2.18-T2         |
| gnu6.3.0-mpt2.18               |
| gnu6.3.0-mvapich2.2            |
| intel18.0.1-mpt2.18            |
| intel18.0.1-mpt2.18            |

** OLD CONUS 12km vs NEW CONUS 12km WRFV3.8.1
   :PROPERTIES:
   :CUSTOM_ID: conus12km_vs_new_conus12km
   :END:
#+header: :var cases=old-vs-new-conus12km-cases labels=old-vs-new-conus12km-labels
#+header: :var title="Scaling results for OLD CONUS 12km vs NEW CONUS 12km with WRFV3.8.1"
#+begin_src ipython :session :ipyfile ./figs/old_vs_new_conus12km_3.svg
plot_scaling(cases, labels, title)
#+end_src

#+RESULTS:
:RESULTS:
# Out[58]:
[[file:./figs/old_vs_new_conus12km_3.svg]]
:END:

** NEW CONUS 12km :noexport:

#+name: new_conus12km-cases
| case          | WRF version | compiler              | mpi            | trial |
|---------------+-------------+-----------------------+----------------+-------|
| new_conus12km | WRFV4.0     | gnu6.3.0              | mpich3.2       | T1    |
| new_conus12km | WRFV4.0     | gnu6.3.0              | mpt2.18        | T1    |
| new_conus12km | WRFV4.0     | gnu8.1.0              | mpt2.18        | T1    |
| new_conus12km | WRFV4.0     | gnu8.1.0              | mvapich2.2     | T1    |
| new_conus12km | WRFV4.0     | gnu8.1.0              | openmpi3.1.0   | T1    |
| new_conus12km | WRFV4.0     | gnu8.1.0_O3           | mvapich2.2     | T1    |
| new_conus12km | WRFV4.0     | gnu8.1.0_fma          | mvapich2.2     | T1    |
| new_conus12km | WRFV4.0     | gnu8.1.0_O3_fma       | mvapich2.2     | T1    |
| new_conus12km | WRFV4.0     | gnu8.1.0_O3_native    | mpt2.18        | T1    |
| new_conus12km | WRFV4.0     | gnu8.1.0_Ofast        | mpt2.18        | T1    |
| new_conus12km | WRFV4.0     | gnu8.1.0_Ofast_native | mpt2.18        | T1    |
| new_conus12km | WRFV4.0     | intel18.0.1           | mpt2.18        | T1    |
| new_conus12km | WRFV4.0     | intel18.0.1           | openmpi3.1.0   | T1    |
| new_conus12km | WRFV4.0     | intel18.0.1           | impi2018.1.163 | T1    |
| new_conus12km | WRFV4.0     | intel18.0.1_host      | mpt2.18        | T1    |
| new_conus12km | WRFV4.0     | intel18.0.1_opt66     | mpt2.18        | T1    |

#+name: new_conus12km-labels
| label                           |
|---------------------------------|
| gnu6.3.0 - mpich3.2             |
| gnu6.3.0 - mpt2.18              |
| gnu8.1.0 - mpt2.18              |
| gnu8.1.0 - mvapich2.2           |
| gnu8.1.0 - openmpi3.1.0         |
| gnu8.1.0_O3 - mvapich2.2        |
| gnu8.1.0_fma - mvapich2.2       |
| gnu8.1.0_O3_fma - mvapich2.2    |
| gnu8.1.0_O3_native - mpt2.18    |
| gnu8.1.0_Ofast - mpt2.18        |
| gnu8.1.0_Ofast_native - mpt2.18 |
| intel18.0.1 - mpt2.18           |
| intel18.0.1  - openmpi3.1.0     |
| intel18.0.1  - impi2018.1.163   |
| intel18.0.1_host - mpt2.18      |
| intel18.0.1_opt66 - mpt2.18     |

** NEW CONUS 12km
   :PROPERTIES:
   :CUSTOM_ID: new_conus12km
   :END:
#+header: :var cases=new_conus12km-cases labels=new_conus12km-labels
#+header: :var title="Scaling results for NEW CONUS 12km with WRFV4.0"
#+begin_src ipython :session :ipyfile ./figs/new_conus12km.svg
plot_scaling(cases, labels, title)
#+end_src

#+RESULTS:
:RESULTS:
# Out[89]:
[[file:./figs/new_conus12km.svg]]
:END:

** NEW CONUS 2.5km :noexport:
#+name: new_conus2.5km-cases
| case          | WRF version | compiler              | mpi            | trial |
|---------------+-------------+-----------------------+----------------+-------|
| new_conus12km | WRFV4.0     | gnu6.3.0              | mpich3.2       | T1    |
| new_conus12km | WRFV4.0     | gnu6.3.0              | mvapich2.2     | T1    |
| new_conus12km | WRFV4.0     | gnu8.1.0              | mpt2.18        | T1    |
| new_conus12km | WRFV4.0     | gnu8.1.0              | mvapich2.2     | T1    |
| new_conus12km | WRFV4.0     | gnu8.1.0              | openmpi3.1.0   | T1    |
| new_conus12km | WRFV4.0     | gnu8.1.0_O3           | mvapich2.2     | T1    |
| new_conus12km | WRFV4.0     | gnu8.1.0_O3_fma       | mvapich2.2     | T1    |
| new_conus12km | WRFV4.0     | gnu8.1.0_O3_native    | mpt2.18        | T1    |
| new_conus12km | WRFV4.0     | gnu8.1.0_Ofast        | mpt2.18        | T1    |
| new_conus12km | WRFV4.0     | gnu8.1.0_Ofast_native | mpt2.18        | T1    |
| new_conus12km | WRFV4.0     | intel18.0.1           | mpt2.18        | T1    |
| new_conus12km | WRFV4.0     | intel18.0.1           | openmpi3.1.0   | T1    |
| new_conus12km | WRFV4.0     | intel18.0.1           | impi2018.1.163 | T1    |
| new_conus12km | WRFV4.0     | intel18.0.1_host      | mpt2.18        | T1    |

#+name: new_conus2.5km-labels
| label                           |
|---------------------------------|
| gnu6.3.0 - mpich3.2             |
| gnu6.3.0 - mvapich2.2           |
| gnu8.1.0 - mpt2.18              |
| gnu8.1.0 - mvapich2.2           |
| gnu8.1.0 - openmpi3.1.0         |
| gnu8.1.0_O3 - mvapich2.2        |
| gnu8.1.0_O3_fma - mvapich2.2    |
| gnu8.1.0_O3_native - mpt2.18    |
| gnu8.1.0_Ofast - mpt2.18        |
| gnu8.1.0_Ofast_native - mpt2.18 |
| intel18.0.1 - mpt2.18           |
| intel18.0.1  - openmpi3.1.0     |
| intel18.0.1  - impi2018.1.163   |
| intel18.0.1_host - mpt2.18      |

** NEW CONUS 2.5km
   :PROPERTIES:
   :CUSTOM_ID: new_conus2.5km
   :END:
#+header: :var cases=new_conus2.5km-cases labels=new_conus2.5km-labels
#+header: :var title="Scaling results for NEW CONUS 2.5km with WRFV4.0"
#+begin_src ipython :session :ipyfile ./figs/new_conus2-5km.svg
plot_scaling(cases, labels, title)
#+end_src

#+RESULTS:
:RESULTS:
# Out[5]:
[[file:./figs/new_conus2-5km.svg]]
:END:

** Maria 3km :noexport:

#+name: maria3km-cases
| case     | WRF version | compiler        | mpi        | trial |
|----------+-------------+-----------------+------------+-------|
| maria3km | WRFV4.0     | gnu6.3.0        | mvapich2.2 | T1    |
| maria3km | WRFV4.0     | gnu8.1.0        | mpt2.18    | T1    |
| maria3km | WRFV4.0     | gnu8.1.0        | mvapich2.2 | T1    |
| maria3km | WRFV4.0     | gnu8.1.0_O3_fma | mvapich2.2 | T1    |
| maria3km | WRFV4.0     | intel18.0.1     | mpt2.18    | T1    |

#+name: maria3km-labels
| label                      |
|----------------------------|
| gnu6.3.0-mvapich2.2        |
| gnu8.1.0-mpt2.18           |
| gnu8.1.0-mvapich2.2        |
| gnu8.1.0-O3-fma-mvapich2.2 |
| intel18.0.1-mpt2.18        |

** Maria 3km
   :PROPERTIES:
   :CUSTOM_ID: maria3km
   :END:
#+header: :var cases=maria3km-cases labels=maria3km-labels
#+header: :var title="Scaling results for Maria 3km with WRFV4.0"
#+begin_src ipython :session :ipyfile ./figs/maria3km.svg
plot_scaling(cases, labels, title)
#+end_src

#+RESULTS:
:RESULTS:
# Out[63]:
[[file:./figs/maria3km.svg]]
:END:

** Maria 1km
   :PROPERTIES:
   :CUSTOM_ID: maria1km
   :END:

#+begin_src ipython :session :ipyfile ./figs/maria1km_runtime.svg
maria_case = ['maria1km', 'WRFV4.0', 'intel18.0.1', 'mpt2.18',  'T1' ]
runs = getFromDict(data, maria_case)
cpus = np.array([d[7]*d[8] for _,d in runs.items()])
p = cpus.argsort()
ticks = np.arange(len(cpus))
comp = np.array([d[1] for _,d in runs.items()])/60.0
init = np.array([d[3] for _,d in runs.items()])/60.0
write = np.array([d[5] for _,d in runs.items()])/60.0
tot = comp+init+write
gridpoints = e_we[case[0]] * e_sn[case[0]]
plt.figure(figsize=(4,4))
plt.plot(ticks, tot[p], 'bo', label='Total')
plt.plot(ticks, init[p], 'rs', label='Initialization')
plt.plot(ticks, comp[p], 'gv', label='Computing')
plt.plot(ticks, write[p], 'b+', label='Writing output')
plt.xticks(ticks, cpus[p], rotation='vertical')
plt.xlabel("Cores", fontweight='bold')
plt.ylabel("Wall-clock time (minutes)", fontweight='bold')
plt.legend(loc='best')
# plt.title("Run time results")
# plt.axis('equal')
# plt.axes().set_aspect('equal', 'datalim')
plt.grid(False)
#+end_src

#+RESULTS:
:RESULTS:
# Out[175]:
[[file:./figs/maria1km_runtime.svg]]
:END:

** Cases table :noexport:
#+name: cases-cases
| case           | WRF version | compiler    | mpi     | trial                                                      |
|----------------+-------------+-------------+---------+------------------------------------------------------------|
| new_conus12km  | WRFV4.0     | intel18.0.1 | mpt2.18 | T1,T1_p18,T1_p09,T1_p04,T1_p02,T1_p01                      |
| new_conus2.5km | WRFV4.0     | intel18.0.1 | mpt2.18 | T1,T1_hm,T1_hm_p18,T1_hm_p09,T1_hm_p04,T1_hm_p02,T1_hm_p01 |
| maria3km       | WRFV4.0     | intel18.0.1 | mpt2.18 | T1,T1_hm,T1_hm_p18,T1_hm_p09,T1_hm_p04,T1_hm_p02,T1_hm_p01 |
| maria1km       | WRFV4.0     | intel18.0.1 | mpt2.18 | T1,T1_hm                                                   |

#+name: cases-labels
| label       |
|-------------|
| CONUS 12km  |
| CONUS 2.5km |
| Maria 3km   |
| Maria 1km   |

** Cases
   :PROPERTIES:
   :CUSTOM_ID: cases
   :END:
#+header: :var cases=cases-cases labels=cases-labels
#+header: :var title=""
#+begin_src ipython :session :ipyfile ./figs/cases.svg
plot_scaling(cases, labels, title, line=True,
             legend_outside=False, fmts=['bo', 'gx', 'r+', 'y^'])
#+end_src

#+attr_latex: :width 1.0\linewidth
#+RESULTS:
:RESULTS:
# Out[177]:
[[file:./figs/cases.svg]]
:END:

** MPIs :noexport:
#+name: new_conus12km-cases-mpi
| case          | WRF version | compiler    | mpi            | trial |
|---------------+-------------+-------------+----------------+-------|
| new_conus12km | WRFV4.0     | gnu6.3.0    | mpich3.2       | T1    |
| new_conus12km | WRFV4.0     | gnu6.3.0    | mpt2.18        | T1    |
| new_conus12km | WRFV4.0     | gnu8.1.0    | mpt2.18        | T1    |
| new_conus12km | WRFV4.0     | gnu8.1.0    | mvapich2.2     | T1    |
| new_conus12km | WRFV4.0     | gnu8.1.0    | openmpi3.1.0   | T1    |
| new_conus12km | WRFV4.0     | intel18.0.1 | mpt2.18        | T1    |
| new_conus12km | WRFV4.0     | intel18.0.1 | openmpi3.1.0   | T1    |
| new_conus12km | WRFV4.0     | intel18.0.1 | impi2018.1.163 | T1    |

#+name: new_conus2.5km-cases-mpi
| case           | WRF version | compiler    | mpi            | trial |
|----------------+-------------+-------------+----------------+-------|
| new_conus2.5km | WRFV4.0     | gnu6.3.0    | mpich3.2       | T1    |
| new_conus2.5km | WRFV4.0     | gnu6.3.0    | mpt2.18        | T1    |
| new_conus2.5km | WRFV4.0     | gnu8.1.0    | mpt2.18        | T1    |
| new_conus2.5km | WRFV4.0     | gnu8.1.0    | mvapich2.2     | T1    |
| new_conus2.5km | WRFV4.0     | gnu8.1.0    | openmpi3.1.0   | T1    |
| new_conus2.5km | WRFV4.0     | intel18.0.1 | mpt2.18        | T1    |
| new_conus2.5km | WRFV4.0     | intel18.0.1 | openmpi3.1.0   | T1    |
| new_conus2.5km | WRFV4.0     | intel18.0.1 | impi2018.1.163 | T1    |

#+name: new_conus12km-labels-mpi
| label                         |
|-------------------------------|
| gnu6.3.0 - mpich3.2           |
| gnu6.3.0 - mpt2.18            |
| gnu8.1.0 - mpt2.18            |
| gnu8.1.0 - mvapich2.2         |
| gnu8.1.0 - openmpi3.1.0       |
| intel18.0.1 - mpt2.18         |
| intel18.0.1  - openmpi3.1.0   |
| intel18.0.1  - impi2018.1.163 |

#+name: new_conus2.5km-labels-mpi
| label                         |
|-------------------------------|
| gnu6.3.0 - mpich3.2           |
| gnu6.3.0 - mpt2.18            |
| gnu8.1.0 - mpt2.18            |
| gnu8.1.0 - mvapich2.2         |
| gnu8.1.0 - openmpi3.1.0       |
| intel18.0.1 - mpt2.18         |
| intel18.0.1  - openmpi3.1.0   |
| intel18.0.1  - impi2018.1.163 |

** MPIs
   :PROPERTIES:
   :CUSTOM_ID: mpis
   :END:
#+header: :var cases=new_conus12km-cases-mpi labels=new_conus12km-labels-mpi
#+header: :var title="Scaling results for NEW CONUS 12km with WRFV4.0"
#+begin_src ipython :session :ipyfile ./figs/new_conus12km_mpi.svg
plot_scaling(cases, labels, title)
#+end_src

#+RESULTS:
:RESULTS:
# Out[64]:
[[file:./figs/new_conus12km_mpi.svg]]
:END:

#+header: :var cases=new_conus2.5km-cases-mpi labels=new_conus2.5km-labels-mpi
#+header: :var title="Scaling results for NEW CONUS 2.5km with WRFV4.0"
#+begin_src ipython :session :ipyfile ./figs/new_conus2-5km_mpi.svg
plot_scaling(cases, labels, title)
#+end_src

#+RESULTS:
:RESULTS:
# Out[65]:
[[file:./figs/new_conus2-5km_mpi.svg]]
:END:

#+header: :var cases=new_conus12km-cases-mpi labels=new_conus12km-labels-mpi
#+header: :var title="Performance for normal number of nodes"
#+begin_src ipython :session :ipyfile ./figs/new_conus12km_bar_normal.svg
plot_bar(cases, 16, labels, title)
#+end_src

#+RESULTS:
:RESULTS:
# Out[53]:
[[file:./figs/new_conus12km_bar_normal.svg]]
:END:

#+header: :var cases=new_conus2.5km-cases-mpi labels=new_conus2.5km-labels-mpi
#+header: :var title="Performance for large number of nodes"
#+begin_src ipython :session :ipyfile ./figs/new_conus12km_bar_large.svg
plot_bar(cases, 4, labels, title)
#+end_src

#+RESULTS:
:RESULTS:
# Out[67]:
[[file:./figs/new_conus12km_bar_large.svg]]
:END:

** Compilers :noexport:
#+name: new_conus12km-cases-compiler
| case          | WRF version | compiler              | mpi        | trial |
|---------------+-------------+-----------------------+------------+-------|
| new_conus12km | WRFV4.0     | gnu8.1.0              | mpt2.18    | T1    |
| new_conus12km | WRFV4.0     | gnu8.1.0              | mvapich2.2 | T1    |
| new_conus12km | WRFV4.0     | gnu8.1.0_O3           | mvapich2.2 | T1    |
| new_conus12km | WRFV4.0     | gnu8.1.0_fma          | mvapich2.2 | T1    |
| new_conus12km | WRFV4.0     | gnu8.1.0_O3_fma       | mvapich2.2 | T1    |
| new_conus12km | WRFV4.0     | gnu8.1.0_O3_native    | mpt2.18    | T1    |
| new_conus12km | WRFV4.0     | gnu8.1.0_Ofast        | mpt2.18    | T1    |
| new_conus12km | WRFV4.0     | gnu8.1.0_Ofast_native | mpt2.18    | T1    |
| new_conus12km | WRFV4.0     | intel18.0.1           | mpt2.18    | T1    |
| new_conus12km | WRFV4.0     | intel18.0.1_host      | mpt2.18    | T1    |
| new_conus12km | WRFV4.0     | intel18.0.1_opt66     | mpt2.18    | T1    |

#+name: new_conus12km-labels-compiler
| label                           |
|---------------------------------|
| gnu8.1.0 - mpt2.18              |
| gnu8.1.0 - mvapich2.2           |
| gnu8.1.0_O3 - mvapich2.2        |
| gnu8.1.0_fma - mvapich2.2       |
| gnu8.1.0_O3_fma - mvapich2.2    |
| gnu8.1.0_O3_native - mpt2.18    |
| gnu8.1.0_Ofast - mpt2.18        |
| gnu8.1.0_Ofast_native - mpt2.18 |
| intel18.0.1 - mpt2.18           |
| intel18.0.1_host - mpt2.18      |
| intel18.0.1_opt66 - mpt2.18     |

#+name: new_conus2.5km-cases-compiler
| case           | WRF version | compiler              | mpi        | trial |
|----------------+-------------+-----------------------+------------+-------|
| new_conus2.5km | WRFV4.0     | gnu8.1.0              | mpt2.18    | T1    |
| new_conus2.5km | WRFV4.0     | gnu8.1.0              | mvapich2.2 | T1    |
| new_conus2.5km | WRFV4.0     | gnu8.1.0_O3           | mvapich2.2 | T1    |
| new_conus2.5km | WRFV4.0     | gnu8.1.0_fma          | mvapich2.2 | T1    |
| new_conus2.5km | WRFV4.0     | gnu8.1.0_O3_fma       | mvapich2.2 | T1    |
| new_conus2.5km | WRFV4.0     | gnu8.1.0_O3_native    | mpt2.18    | T1    |
| new_conus2.5km | WRFV4.0     | gnu8.1.0_Ofast        | mpt2.18    | T1    |
| new_conus2.5km | WRFV4.0     | gnu8.1.0_Ofast_native | mpt2.18    | T1    |
| new_conus2.5km | WRFV4.0     | intel18.0.1           | mpt2.18    | T1    |
| new_conus2.5km | WRFV4.0     | intel18.0.1_host      | mpt2.18    | T1    |

#+name: new_conus2.5km-labels-compiler
| label                           |
|---------------------------------|
| gnu8.1.0 - mpt2.18              |
| gnu8.1.0 - mvapich2.2           |
| gnu8.1.0_O3 - mvapich2.2        |
| gnu8.1.0_fma - mvapich2.2       |
| gnu8.1.0_O3_fma - mvapich2.2    |
| gnu8.1.0_O3_native - mpt2.18    |
| gnu8.1.0_Ofast - mpt2.18        |
| gnu8.1.0_Ofast_native - mpt2.18 |
| intel18.0.1 - mpt2.18           |
| intel18.0.1_host - mpt2.18      |

#+name: new_conus12km-cases-gnu
| case          | WRF version | compiler              | mpi          | trial |
|---------------+-------------+-----------------------+--------------+-------|
| new_conus12km | WRFV4.0     | gnu6.3.0              | mpich3.2     | T1    |
| new_conus12km | WRFV4.0     | gnu6.3.0              | mpt2.18      | T1    |
| new_conus12km | WRFV4.0     | gnu8.1.0              | mpt2.18      | T1    |
| new_conus12km | WRFV4.0     | gnu8.1.0              | mvapich2.2   | T1    |
| new_conus12km | WRFV4.0     | gnu8.1.0              | openmpi3.1.0 | T1    |
| new_conus12km | WRFV4.0     | gnu8.1.0_O3           | mvapich2.2   | T1    |
| new_conus12km | WRFV4.0     | gnu8.1.0_fma          | mvapich2.2   | T1    |
| new_conus12km | WRFV4.0     | gnu8.1.0_O3_fma       | mvapich2.2   | T1    |
| new_conus12km | WRFV4.0     | gnu8.1.0_O3_native    | mpt2.18      | T1    |
| new_conus12km | WRFV4.0     | gnu8.1.0_Ofast        | mpt2.18      | T1    |
| new_conus12km | WRFV4.0     | gnu8.1.0_Ofast_native | mpt2.18      | T1    |

#+name: new_conus12km-labels-gnu
| label                           |
|---------------------------------|
| gnu6.3.0 - mpich3.2             |
| gnu6.3.0 - mpt2.18              |
| gnu8.1.0 - mpt2.18              |
| gnu8.1.0 - mvapich2.2           |
| gnu8.1.0 - openmpi3.1.0         |
| gnu8.1.0_O3 - mvapich2.2        |
| gnu8.1.0_fma - mvapich2.2       |
| gnu8.1.0_O3_fma - mvapich2.2    |
| gnu8.1.0_O3_native - mpt2.18    |
| gnu8.1.0_Ofast - mpt2.18        |
| gnu8.1.0_Ofast_native - mpt2.18 |

#+name: new_conus12km-cases-intel
| case          | WRF version | compiler          | mpi            | trial |
|---------------+-------------+-------------------+----------------+-------|
| new_conus12km | WRFV4.0     | intel18.0.1       | mpt2.18        | T1    |
| new_conus12km | WRFV4.0     | intel18.0.1       | mpt2.18        | T2    |
| new_conus12km | WRFV4.0     | intel18.0.1_host  | mpt2.18        | T1    |
| new_conus12km | WRFV4.0     | intel18.0.1_opt66 | mpt2.18        | T1    |
| new_conus12km | WRFV4.0     | intel18.0.1       | openmpi3.1.0   | T1    |
| new_conus12km | WRFV4.0     | intel18.0.1       | impi2018.1.163 | T1    |

#+name: new_conus12km-labels-intel
| label                         |
|-------------------------------|
| intel18.0.1 - mpt2.18         |
| intel18.0.1 - mpt2.18         |
| intel18.0.1_host - mpt2.18    |
| intel18.0.1_opt66 - mpt2.18   |
| intel18.0.1  - openmpi3.1.0   |
| intel18.0.1  - impi2018.1.163 |


** Compilers
   :PROPERTIES:
   :CUSTOM_ID: compilers
   :END:
#+header: :var cases=new_conus12km-cases-compiler labels=new_conus12km-labels-compiler
#+header: :var title="Scaling results for NEW CONUS 12km with WRFV4.0"
#+begin_src ipython :session :ipyfile ./figs/new_conus12km_compiler.svg
plot_scaling(cases, labels, title)
#+end_src

#+RESULTS:
:RESULTS:
# Out[90]:
[[file:./figs/new_conus12km_compiler.svg]]
:END:

#+header: :var cases=new_conus2.5km-cases-compiler labels=new_conus2.5km-labels-compiler
#+header: :var title="Scaling results for NEW CONUS 2.5km with WRFV4.0"
#+begin_src ipython :session :ipyfile ./figs/new_conus2-5km_compiler.svg
plot_scaling(cases, labels, title)
#+end_src

#+RESULTS:
:RESULTS:
# Out[83]:
[[file:./figs/new_conus2-5km_compiler.svg]]
:END:

#+header: :var cases=new_conus12km-cases-gnu labels=new_conus12km-labels-gnu
#+header: :var title="Scaling results for NEW CONUS 12km with WRFV4.0"
#+begin_src ipython :session :ipyfile ./figs/new_conus12km_gnu.svg
plot_scaling(cases, labels, title)
#+end_src

#+RESULTS:
:RESULTS:
# Out[84]:
[[file:./figs/new_conus12km_gnu.svg]]
:END:

#+header: :var cases=new_conus12km-cases-intel labels=new_conus12km-labels-intel
#+header: :var title="Scaling results for NEW CONUS 12km with WRFV4.0"
#+begin_src ipython :session :ipyfile ./figs/new_conus12km_intel.svg
plot_scaling(cases, labels, title)
#+end_src

#+RESULTS:
:RESULTS:
# Out[91]:
[[file:./figs/new_conus12km_intel.svg]]
:END:

** Hybrid :noexport:
#+name: cases-omp
| case           | WRF version | compiler    | mpi         | trial     |
|----------------+-------------+-------------+-------------+-----------|
| new_conus12km  | WRFV4.0     | intel18.0.1 | mpt2.18     | T1        |
| new_conus12km  | WRFV4.0     | intel18.0.1 | mpt2.18_omp | T1_m2_o18 |
| new_conus12km  | WRFV4.0     | intel18.0.1 | mpt2.18_omp | T1_m3_o12 |
| new_conus12km  | WRFV4.0     | intel18.0.1 | mpt2.18_omp | T1_m4_o9  |
| new_conus12km  | WRFV4.0     | intel18.0.1 | mpt2.18_omp | T1_m6_o6  |
| new_conus2.5km | WRFV4.0     | intel18.0.1 | mpt2.18     | T1        |
| new_conus2.5km | WRFV4.0     | intel18.0.1 | mpt2.18_omp | T1_m2_o18 |
| new_conus2.5km | WRFV4.0     | intel18.0.1 | mpt2.18_omp | T1_m3_o12 |
| new_conus2.5km | WRFV4.0     | intel18.0.1 | mpt2.18_omp | T1_m4_o9  |
| new_conus2.5km | WRFV4.0     | intel18.0.1 | mpt2.18_omp | T1_m6_o6  |


#+name: labels-omp
| label                          |
|--------------------------------|
| new_conus12km                  |
| new_conus12km - 2 MPI, 18 OMP  |
| new_conus12km - 3 MPI, 12 OMP  |
| new_conus12km - 4 MPI, 9 OMP   |
| new_conus12km - 6 MPI, 6 OMP   |
| new_conus2.5km                 |
| new_conus2.5km - 2 MPI, 18 OMP |
| new_conus2.5km - 3 MPI, 12 OMP |
| new_conus2.5km - 4 MPI, 9 OMP  |
| new_conus2.5km - 6 MPI, 6 OMP  |

** Hybrid
   :PROPERTIES:
   :CUSTOM_ID: hybrid
   :END:
#+header: :var cases=cases-omp labels=labels-omp
#+header: :var title="Scaling results for Hybrid WRFV4.0"
#+begin_src ipython :session :ipyfile ./figs/hybrid.svg
plot_scaling(cases, labels, title, shifts=[1, 18, 12, 9, 6,  1, 18, 12, 9, 6])
#+end_src

#+RESULTS:
:RESULTS:
# Out[75]:
[[file:./figs/hybrid.svg]]
:END:

#+header: :var cases=cases-omp labels=labels-omp
#+header: :var title="Performance for normal number of nodes"
#+begin_src ipython :session :ipyfile ./figs/hybrid_bar_normal.svg
cases=cases[:5]; labels=labels[:5]
plot_bar(cases, 16, labels, title)
#+end_src

#+RESULTS:
:RESULTS:
# Out[76]:
[[file:./figs/hybrid_bar_normal.svg]]
:END:

#+header: :var cases=cases-omp labels=labels-omp
#+header: :var title="Performance for large number of nodes"
#+begin_src ipython :session :ipyfile ./figs/hybrid_bar_large.svg
cases=cases[5:]; labels=labels[5:]
plot_bar(cases, 8, labels, title)
#+end_src

#+RESULTS:
:RESULTS:
# Out[77]:
[[file:./figs/hybrid_bar_large.svg]]
:END:

** Katrina :noexport:
#+name: katrina1km-cases
| case       | WRF version | compiler    | mpi        | trial |
|------------+-------------+-------------+------------+-------|
| katrina1km | WRFV3.9.1.1 | gnu6.3.0    | mpt2.18    | D1    |
| katrina1km | WRFV3.9.1.1 | gnu8.1.0    | mpt2.18    | D1    |
| katrina1km | WRFV3.9.1.1 | gnu8.1.0    | mvapich2.2 | D1    |
| katrina1km | WRFV3.9.1.1 | intel17.0.1 | mpt2.18    | D1    |
| katrina1km | WRFV3.9.1.1 | intel18.0.1 | mpt2.18    | D1    |

#+name: katrina3km-cases
| katrina3km | WRFV3.9.1.1 | gnu6.3.0    | mpt2.18    | D1    |
| katrina3km | WRFV3.9.1.1 | gnu8.1.0    | mpt2.18    | D1    |
| katrina3km | WRFV3.9.1.1 | gnu8.1.0    | mvapich2.2 | D1    |
| katrina3km | WRFV3.9.1.1 | intel18.0.1 | mpt2.18    | D1    |

#+name: katrina1km-labels
| label                       |
|-----------------------------|
| 1km - gnu6.3.0 - mpt2.18    |
| 1km - gnu8.1.0 - mpt2.18    |
| 1km - gnu8.1.0 - mvapich2.2 |
| 1km - intel17.0.1 - mpt2.18 |
| 1km - intel18.0.1 - mpt2.18 |

#+name: katrina3km-labels
| 3km - gnu6.3.0 - mpt2.18    |
| 3km - gnu8.1.0 - mpt2.18    |
| 3km - gnu8.1.0 - mvapich2.2 |
| 3km - intel18.0.1 - mpt2.18 |

** Katrina
   :PROPERTIES:
   :CUSTOM_ID: katrina
   :END:
#+header: :var cases=katrina3km-cases labels=katrina3km-labels
#+header: :var title="Katrina WRFV3.9.1.1"
#+begin_src ipython :session :ipyfile ./figs/katrina.svg
plot_scaling(cases, labels, title)
#+end_src

#+attr_latex: :width 1.0\linewidth
#+RESULTS:
:RESULTS:
# Out[41]:
[[file:./figs/katrina.svg]]
:END:

#+header: :var cases=katrina1km-cases labels=katrina1km-labels
#+header: :var title="Katrina WRFV3.9.1.1"
#+begin_src ipython :session :ipyfile ./figs/katrina.svg
plot_scaling(cases, labels, title, )
#+end_src

** Cases plus   :noexport:
#+name: casesplus-cases
| case           | WRF version | compiler    | mpi        | trial                                                      |
|----------------+-------------+-------------+------------+------------------------------------------------------------|
| new_conus12km  | WRFV4.0     | intel18.0.1 | mpt2.18    | T1,T1_p18,T1_p09,T1_p04,T1_p02,T1_p01                      |
| new_conus2.5km | WRFV4.0     | intel18.0.1 | mpt2.18    | T1,T1_hm,T1_hm_p18,T1_hm_p09                               |
| maria3km       | WRFV4.0     | intel18.0.1 | mpt2.18    | T1,T1_hm,T1_hm_p18,T1_hm_p09,T1_hm_p04,T1_hm_p02,T1_hm_p01 |
| maria1km       | WRFV4.0     | intel18.0.1 | mpt2.18    | T1,T1_hm                                                   |
| katrina1km     | WRFV3.9.1.1 | intel18.0.1 | mpt2.18    | D1                                                         |
| katrina3km     | WRFV3.9.1.1 | intel18.0.1 | mpt2.18    | D1                                                         |

#+name: casesplus-labels
| label          |
|----------------|
| new_conus12km  |
| new_conus2.5km |
| maria3km       |
| maria1km       |
| katrina1km     |
| katrina3km     |

** Cases plus
   :PROPERTIES:
   :CUSTOM_ID: cases_plus
   :END:
#+header: :var cases=casesplus-cases labels=casesplus-labels
#+header: :var title=""
#+begin_src ipython :session :ipyfile ./figs/casesplus.svg
plot_scaling(cases, labels, title, legend_outside=False)
#+end_src

#+attr_latex: :width 1.0\linewidth
#+RESULTS:
:RESULTS:
# Out[8]:
[[file:./figs/casesplus.svg]]
:END:

** mvapich i/o
   :PROPERTIES:
   :CUSTOM_ID: mvapich_io
   :END:

#+begin_note
mvapich2.3 BIND corresponds to setting the environment variables:
~MV2_CPU_BINDING_POLICY=hybrid MV2_HYBRID_BINDING_POLICY=bunch~

mvapich2.3 HW corresponds to setting the environment variables:
~MV2_CPU_BINDING_POLICY=hybrid MV2_HYBRID_BINDING_POLICY=bunch MV2_USE_MCAST=1 MV2_ENABLE_SHARP=1~
#+end_note

#+begin_src ipython :session
def plot_mvapich(case):
    wrf, comp = 'WRFV4.0', 'gnu8.1.0'
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(7,3.5))
    fig.tight_layout()
    for mpi,trial,l,f in [('mpt2.18', 'T1', '','o'), ('mvapich2.2', 'T1', '', 'x'),
                        ('mvapich2.3', 'E0_T1', ' BIND', '+'), ('mvapich2.3', 'E1_T1', ' HW', '.')]:
        run = getFromDict(data, [case, wrf, comp, mpi , trial])
        ax1.plot([d[7]*d[8] for _,d in run.items()], [d[4] for _,d in run.items()],
                f+'--', label=mpi+l)
        ax2.plot([d[7]*d[8] for _,d in run.items()], [d[6] for _,d in run.items()],
                f+'--', label=mpi+l)

    ax1.set_xlabel("Cores", fontweight='bold')
    ax1.set_ylabel("Init Time (minutes)", fontweight='bold')
    ax2.set_xlabel("Cores", fontweight='bold')
    ax2.set_ylabel("Write Time (minutes)", fontweight='bold', labelpad=0)
    #plt.legend(loc='upper center', bbox_to_anchor=(1.38, 1.0), ncol=1)
    plt.legend(loc='upper center', bbox_to_anchor=(-0.15, -0.2), ncol=4)
#+end_src

#+RESULTS:
:RESULTS:
# Out[272]:
:END:

#+begin_src ipython :session :ipyfile ./figs/mvapich-io-new_conus12km.svg
plot_mvapich('new_conus12km')
#+end_src

#+RESULTS:
:RESULTS:
# Out[273]:
[[file:./figs/mvapich-io-new_conus12km.svg]]
:END:

#+begin_src ipython :session :ipyfile ./figs/mvapich-io-maria3km.svg
plot_mvapich('maria3km')
#+end_src

#+RESULTS:
:RESULTS:
# Out[276]:
[[file:./figs/mvapich-io-maria3km.svg]]
:END:

#+begin_src ipython :session :ipyfile ./figs/mvapich-io-new_conus2-5km.svg
case, wrf, comp, trial = 'new_conus2.5km', 'WRFV4.0', 'gnu8.1.0', 'T1'
for mpi,l in [('mvapich2.2', 'b'), ('mpt2.18', 'r')]:
    run = getFromDict(data, [case, wrf, comp, mpi , trial])
    plt.plot([d[7]*d[8] for _,d in run.items()], [d[4] for _,d in run.items()],
             l+'+--', label='init '+mpi)
    plt.plot([d[7]*d[8] for _,d in run.items()], [d[6] for _,d in run.items()],
             l+'o--', label='write '+mpi)

plt.xlabel("Cores")
plt.ylabel("Time")
plt.legend(loc='best')
plt.title("Scaling of I/O for new_conus2.5km")
plt.grid(False)
#+end_src

#+RESULTS:
:RESULTS:
# Out[91]:
[[file:./figs/mvapich-io-new_conus2-5km.svg]]
:END:

** figs :noexport:

#+name: new_conus12km-cases-gnu-mpi
| case          | WRF version | compiler    | mpi            | trial |
|---------------+-------------+-------------+----------------+-------|
| new_conus12km | WRFV4.0     | gnu8.1.0    | mpt2.18        | T1    |
| new_conus12km | WRFV4.0     | gnu8.1.0    | openmpi3.1.0   | T1    |
| new_conus12km | WRFV4.0     | gnu8.1.0    | mvapich2.2     | T1    |
| new_conus12km | WRFV4.0     | gnu6.3.0    | mpich3.2       | T1    |

#+name: new_conus12km-cases-intel-mpi
| case          | WRF version | compiler    | mpi            | trial |
|---------------+-------------+-------------+----------------+-------|
| new_conus12km | WRFV4.0     | intel18.0.1 | mpt2.18        | T1    |
| new_conus12km | WRFV4.0     | intel18.0.1 | openmpi3.1.0   | T1    |
| new_conus12km | WRFV4.0     | intel18.0.1 | impi2018.1.163 | T1    |

#+name: new_conus12km-labels-gnu-mpi
| label   |
|---------|
| MPT     |
| OpenMPI |
| MVAPICH |
| MPICH   |

#+name: new_conus12km-labels-intel-mpi
| label     |
|-----------|
| MPT       |
| OpenMPI   |
| Intel MPI |

#+name: new_conus12km-cases-compiler-gnu-fig
| case          | WRF version | compiler              | mpi        | trial |
|---------------+-------------+-----------------------+------------+-------|
| new_conus12km | WRFV4.0     | gnu8.1.0              | mpt2.18    | T1    |
| new_conus12km | WRFV4.0     | gnu8.1.0_O3           | mvapich2.2 | T1    |
| new_conus12km | WRFV4.0     | gnu8.1.0_fma          | mvapich2.2 | T1    |
| new_conus12km | WRFV4.0     | gnu8.1.0_O3_fma       | mvapich2.2 | T1    |
| new_conus12km | WRFV4.0     | gnu8.1.0_O3_native    | mpt2.18    | T1    |
| new_conus12km | WRFV4.0     | gnu8.1.0_Ofast        | mpt2.18    | T1    |
| new_conus12km | WRFV4.0     | gnu8.1.0_Ofast_native | mpt2.18    | T1    |

#+name: new_conus12km-cases-compiler-intel-fig
| case          | WRF version | compiler          | mpi     | trial |
|---------------+-------------+-------------------+---------+-------|
| new_conus12km | WRFV4.0     | intel18.0.1       | mpt2.18 | T1    |
| new_conus12km | WRFV4.0     | intel18.0.1_host  | mpt2.18 | T1    |
| new_conus12km | WRFV4.0     | intel18.0.1_opt66 | mpt2.18 | T1    |

#+name: new_conus12km-labels-compiler-fig
| label                         |
|-------------------------------|
| gnu -O2                       |
| gnu -O3                       |
| gnu -mfma                     |
| gnu -O3 -mfma                 |
| gnu -O3 -march=native         |
| gnu -Ofast                    |
| gnu -Ofast -march=native      |
| intel -O3                     |
| intel -Xhost                  |
| intel -xHost -fp-model fast=2 |

** figs
#+header: :var cases=new_conus12km-cases-gnu-mpi labels=new_conus12km-labels-gnu-mpi
#+header: :var title=""
#+begin_src ipython :session :ipyfile ./figs/new_conus12km_gnu_mpi.svg
plot_scaling(cases, labels, title, legend_outside=False, fmts=['C0o--', 'C1x--', 'C2+--', 'C3.--'])
#+end_src

#+RESULTS:
:RESULTS:
# Out[217]:
[[file:./figs/new_conus12km_gnu_mpi.svg]]
:END:

#+header: :var cases=new_conus12km-cases-intel-mpi labels=new_conus12km-labels-intel-mpi
#+header: :var title=""
#+begin_src ipython :session :ipyfile ./figs/new_conus12km_intel_mpi.svg
plot_scaling(cases, labels, title, legend_outside=False, fmts=['C0o--', 'C1x--', 'C4^--'])
#+end_src

#+RESULTS:
:RESULTS:
# Out[219]:
[[file:./figs/new_conus12km_intel_mpi.svg]]
:END:

#+header: :var cases_gnu=new_conus12km-cases-compiler-gnu-fig
#+header: :var cases_intel=new_conus12km-cases-compiler-intel-fig
#+header: :var labels=new_conus12km-labels-compiler-fig
#+begin_src ipython :session :ipyfile ./figs/new_conus12km_bar_compiler_fig.svg
nodes='N002.rsl'
secs_gnu = []
for case in cases_gnu:
    sec = getFromDict(data, case+[nodes])[2]
    secs_gnu.append(sec if isinstance(sec, float) else 0.0)
secs_intel = []
for case in cases_intel:
    sec = getFromDict(data, case+[nodes])[2]
    secs_intel.append(sec if isinstance(sec, float) else 0.0)

secs_gnu = np.array(secs_gnu)
secs_intel = np.array(secs_intel)

y_pos_gnu = np.arange(len(cases_gnu))
y_pos_intel = np.arange(len(cases_intel)) + y_pos_gnu[-1] + 1

fig, ax = plt.subplots(figsize=(5,3.5))

ax.barh(y_pos_gnu, secs_intel[0]/secs_gnu, color='C0')
ax.barh(y_pos_intel, secs_intel[0]/secs_intel, color='C1')

ax.set_yticks([]) #y_pos)
ax.set_xticks(np.linspace(0,1.2,num=13))
# ax.set_yticklabels(labels_gnu+labels_intel)
for i, label in enumerate([l[0] for l in labels]):
    ax.text(0.05, i + .20, label, color='white', fontweight='bold')
ax.invert_yaxis()
#ax.set_xlabel('Time steps/s', fontweight='bold')
ax.set_xlabel('Normalized Performance', fontweight='bold')

# ax.annotate('', xy=(0.72, 0.07),  xycoords='figure fraction',
#             xytext=(0.28, 0.07), textcoords='figure fraction',
#             arrowprops=dict(facecolor='black', width=2))
# plt.figtext(0.1, -0.05, 'worse\nperformance', fontweight='bold')
# plt.figtext(0.75, -0.05, 'better\nperformance', fontweight='bold')
#+end_src

#+RESULTS:
:RESULTS:
# Out[199]:
: Text(0.5,0,'Normalized Performance')
[[file:./figs/new_conus12km_bar_compiler_fig.svg]]
:END:

* Analysis
  :PROPERTIES:
  :CUSTOM_ID: analysis
  :END:
** Compilers
  :PROPERTIES:
  :CUSTOM_ID: analysis-compilers
  :END:

We see from Figure [[wrf_scaling]] that the Intel compiler is consistently faster
than the Gnu compiler across all flags tried. We also see that for both Intel
and Gnu, the ~-Ofast~ (for Gnu) or ~-fp-model fast=2~ (for Intel) are the only
flags that make a significant difference in speed. Other flags tried such as
~-mfma~ or ~-march=native~ ~-Xhost~ made little to no difference in WRF's speed.

#+name: wrf_scaling
#+caption: Comparison of Intel 18.0.1 and Gnu 8.1.0 compilers and various compile flags
#+attr_latex: :width 0.8\linewidth
[[file:./figs/new_conus12km_bar_compiler_fig.svg]]

** MPI Libraries
  :PROPERTIES:
  :CUSTOM_ID: analysis-mpi
  :END:

We also tested the scaling performance of several MPI implementations. Note that
Cheyenne uses a Mellanox EDR InfiniBand high-speed interconnect with a Partial
9D Enhanced Hypercube single-plane interconnect topology.

We see from Figures [[gnu-mpi]] and [[intel-mpi]] that MPT, MVAPICH and OpenMPI all have
similar performance, while MPICH has overall poor performance and the
performance Intel MPI does not scale well to large node counts.

#+caption: MPI comparison with Gnu 8.1.0
#+name: gnu-mpi
#+attr_latex: :width 0.8\linewidth
[[file:./figs/new_conus12km_gnu_mpi.svg]]

#+caption: MPI comparison with Intel 18.0.1
#+name: intel-mpi
#+attr_latex: :width 0.8\linewidth
[[file:./figs/new_conus12km_intel_mpi.svg]]

** Run Time Scaling Comparison
  :PROPERTIES:
  :CUSTOM_ID: run_time_scaling_comparison
  :END:

Figures [[yellowstone-runtime]] and [[cheyenne-runtime]] show the
total run time for WRF using increasing numbers of cores with the total run time
broken down into the initialization time, computation time, and writing time.
These results use the 1 km Hurricane Maria Benchmark case. We see that on
Cheyenne the initialization and writing output times remain relatively fixed,
only increasing slightly as you move to larger core counts. However on
Yellowstone, the initialization time scaled much poorer at large node counts,
eventually leading to unfeasible long jobs. This improvement in the scaling of
the initialization time is likely due to the improvements made to WRF's
initialization code with how the MPI calls are performed along with improvements
in the MPI used in Cheyenne versus Yellowstone.

#+caption: Run Time Scaling on *Yellowstone*
#+name: yellowstone-runtime
#+attr_latex: :width 0.8\linewidth
[[file:./imgs/timingdav.png]]

#+caption: Run Time Scaling on *Cheyenne*
#+name: cheyenne-runtime
#+attr_latex: :width 0.8\linewidth
[[file:./figs/maria1km_runtime.svg]]

** Computation Time Scaling Results
  :PROPERTIES:
  :CUSTOM_ID: comp_time_scaling_comparison
  :END:

When expressing the scaling as a function of WRF gridpoints per core, we see
that all the cases scale similarly in both Figure [[cheyenne-scaling]] on
Cheyenne and Figure [[yellowstone-scaling]] on Yellowstone. Note that both
axes are logarithmic, so a small distance between points corresponds to a large
difference in values. On both Cheyenne and Yellowstone, in the high relative
gridpoints per core region, we see that WRF has linear *strong scaling*. This
means that WRF is making effective use of the parallel computational resources
available to it. So increasing the number of cores a run uses, will
proportionately decrease WRF's computation time (however initialization and I/O
time may increase) while about same number of total core-hours will be used for
computation.

#+caption: Computation Scaling on *Yellowstone*
#+name: yellowstone-scaling
#+attr_latex: :width 0.8\linewidth
[[file:./imgs/scalingideppresdav.png]]

#+caption: Computation Scaling on *Cheyenne*
#+name: cheyenne-scaling
#+attr_latex: :width 0.8\linewidth
[[file:./figs/cases.svg]]


Running WRF on Cheyenne versus Yellowstone differs in the the low relative
gridpoints per core region. On Yellowstone we see WRF depart from the linear
strong scaling relationship. User's running WRF in this low gridpoints per core
region on Yellowstone would effectively be using more core-hours to run the same
simulation than if they had run it on fewer cores. In this low gridpoints per
core region, MPI communication starts to dominate the actual time spent in
computation. However on Cheyenne, we see that WRF does not significantly depart
from this linear strong scaling at any amount of gridpoints per core. Likely
this is due to improvements in the WRF's MPI code along with a better network
interconnect on Cheyenne than Yellowstone along with a MPI library. Furthermore
WRFV4.0 will refuse to run if there is a minimum patch size of less than 10 grid
points in either direction. This limits users from running with fewer than 100
gridpoints per core, which would likely be a very MPI communication bound region
where WRF would depart from its linear strong scaling.

The interesting feature in Figure [[cheyenne-scaling]] in the high
gridpoints per core region where the time steps per second seem to jump slightly
is an artifact of the memory constraints on Cheyenne. The normal nodes on
Cheyenne have only 64 GB of memory, which is less than on Yellowstone. WRF runs
with too many gridpoints per node will run out of memory and be killed. The
maximum number of gridpoints per node that will fit into the 64 GB of memory of
that node depends on the physics parameterizations, however, we observed that
typically the maximum is between 10^5 and 10^6 total gridpoints. Thus to obtain
the results in the very large gridpoints per core region, we utilized Cheyenne's
128 GB memory nodes for an additional point or two, then we undersubscribed the
cores on each node. This undersubscription of cores is likely responsible for
the small bump in speed observed. However we do not recommend that users
undersubscribe cores as core-hours are charged for all cores on the node so
undersubscription of cores will be an inefficient use of a user's core-hour
allocation.

Finally it's worth noting that the vertical axis between Figures
[[yellowstone-scaling]] and [[cheyenne-scaling]] is shifted due to
the difference in clock speeds between the processors used in Yellowstone versus
those used in Cheyenne.

* WRF scaling and timing on Cheyenne
  :PROPERTIES:
  :CUSTOM_ID: wrf_scaling_and_timing_on_cheyenne
  :END:

#+begin_note
[[https://www2.cisl.ucar.edu/software/community-models/wrf-scaling-and-timing][The previous recommendations for users of WRF on Yellowstone]]

To see how WRFs performance differs on Cheyenne compared to yellowstone, see the
discussion [[#run_time_scaling_comparison][here]]
#+end_note

The results on Cheyenne below may be helpful in answering such questions as:

- "Is it possible to solve a problem with such-and-such resolution in a timely
  manner?"
- "If I use more cores I will have the results more quickly, but with this
  resolution will my run be in the efficient strong-scaling regime, an
  intermediate one, or in the very inefficient one dominated by I/O and
  initialization instead of computing?"

Numbers from the figures below can help you develop some back-of-the-envelope
estimates of what will happen if you increase or decrease the core counts of
your runs, so that you can find one that is optimal for you both in terms of
time-to-solution and in terms of your allocation.

If you're preparing an allocation request, while these plots provide some
guidance, you do need to run some tests on Cheyenne or a comparable system to
support the validity of your core-hour estimate for your own physics
parameterization, and to make sure you account for the overhead from
initialization and I/O. (Different I/O settings and frequency will affect your
runs differently.)

Also see [[#optimizing_wrf_performance][Optimizing WRF performance]] for related documentation and [[https://www2.cisl.ucar.edu/user-support/determining-computational-resource-needs][Determining
computational resource needs]] for additional information on preparing allocation
requests.

--------------------------------------------------------------------------------

** TODO Scaling Results
   :PROPERTIES:
   :CUSTOM_ID: scaling_results
   :END:

Figure [[wrf_scaling]] shows scaling results from Hurricane Maria simulations at 3km
and 1km resolutions along with CONUS simulations at 12km and 2.5km resolutions
all run using WRFV4.0. When expressed this way, all the cases scale similarly.
Note that both axes are logarithmic, so a small distance between points
corresponds to a large difference in values.

#+name: wrf_scaling
#+caption: WRF scaling
[[file:./figs/cases.svg]]

On Cheyenne, we see that WRF does not significantly depart from this linear
strong scaling at any amount of gridpoints per core.

*** TODO Running WRF with a small number of gridpoints per core

WRFV4.0 will refuse to run if there is a minimum patch size of less than 10 grid
points in either direction. This limits users from running with fewer than 100
gridpoints per core, which would likely be a very MPI communication bound region
where WRF would depart from its linear strong scaling. Thus WRF will prevent you
from running on very large relative core counts where initialization, I/O, and
communication dominate.

*** TODO Running WRF with a large number of gridpoints per core

The interesting feature in Figure [[wrf_scaling]] in the high gridpoints per core
region where the time steps per second seem to jump slightly is an artifact of
the memory constraints on Cheyenne. The normal nodes on Cheyenne have only 64 GB
of memory. WRF runs with too many gridpoints per node will run out of memory and
be killed. The maximum number of gridpoints per node that will fit into the 64
GB of memory of that node depends on the physics parameterizations, however, we
observed that typically the maximum is between 10^5 and 10^6 total gridpoints.
Thus to obtain the results in the very large gridpoints per core region, we
utilized Cheyenne's 128 GB memory nodes for an additional point or two, then we
undersubscribed the cores on each node. This undersubscription of cores is
likely responsible for the small bump in speed observed. However we do not
recommend that users undersubscribe cores as core-hours are charged for all
cores on the node so undersubscription of cores will be an inefficient use of a
user's core-hour allocation.


** Run time results
   :PROPERTIES:
   :CUSTOM_ID: run_time_results
   :END:

Figure [[wrf_runtime]] below shows the /total/ run time for WRF jobs using
increasing numbers of cores with the total run time broken down into the
initialization time, computation time, and writing time.

These results are based on simulations of Hurricane Maria (2017) at 1km
resolution.

As illustrated, initialization and writing output times remain relatively fixed,
only increasing slightly as you move to larger core counts. Times shown are for a
and single output file of the Maria 1km case, which used a domain of about 372
million grid points. If you have more restarts and output files, your numbers
will be different but the trend will be similar.

#+name: wrf_runtime
#+caption: WRF Run Timing
[[file:./figs/maria1km_runtime.svg]]

*** TODO Running WRF on a large number of nodes
Users wishing to run WRF with more than 10,000 MPI tasks, which on Cheyenne
corresponds to using 277 nodes with 36 MPI tasks per node need to patch WRF to
use more digits in the ~rsl.out.XXXX~ and ~rsl.error.XXXX~ files otherwise WRF
will not run. A patch for WRFV4.0 can be [[https://github.com/akirakyle/WRF_benchmarks/blob/master/WRFs/WRFV4.0-rsl-8digit.patch][found here]].


* TODO Optimizing WRF performance
  :PROPERTIES:
  :CUSTOM_ID: optimizing_wrf_performance
  :END:
#+begin_note
[[https://www2.cisl.ucar.edu/software/community-models/optimizing-wrf-performance][The previous recommendations for users of WRF on Yellowstone]]
#+end_note

These recommendations for optimizing the performance of the Weather Research and
Forecasting (WRF) modeling system are based on the results of numerous jobs run
on the Cheyenne system by the CISL Consulting Services Group. The jobs
included small runs and others, with different domain sizes and time steps.

--------------------------------------------------------------------------------

** TODO Compiling and linking
   :PROPERTIES:
   :CUSTOM_ID: compiling_and_linking
   :END:

We recommend using the default *Intel compiler*. In our runs, the Intel compiler
consistently outperformed the GNU compiler by ~1.5x.

and the compiler's default
settings as contained in the script for creating the *configure.wrf* file.

The best results were achieved with a *Distributed-Memory Parallelism (DMPar)*
build, which enables MPI. Depending on the individual case, advanced WRF users
may find some improvement in performance with a hybrid build, using both DMPar
and SMPar.

We do not recommend SMPar alone or serial WRF builds.

--------------------------------------------------------------------------------

** TODO Runtime options
   :PROPERTIES:
   :CUSTOM_ID: runtime_options
   :END:

We recommend using the following when running WRF jobs:

1. Hyper-threading
2. Processor binding

Hyper-threading improved computing performance by 8% in a test MPI case using
256 Yellowstone nodes. In other cases, hyper-threading had negligible impact.

Tests of hybrid MPI/OpenMP jobs, both with and without hyper-threading, showed
that hybrid parallelism can provide marginally higher performance than pure MPI
parallelism, but run-to-run variability was high.

Processor binding was enabled by default when running MPI jobs on Yellowstone.
We used the default binding settings in test runs.

--------------------------------------------------------------------------------

** TODO Scaling and core count
   :PROPERTIES:
   :CUSTOM_ID: scaling_and_core_count
   :END:

WRF is a highly scalable model, demonstrating both weak and strong scaling,
within limits defined by the problem size. We do not recommend running WRF on
extremely large core counts (relative to the number of grid points in the
domain). This is because there will be increasingly less speed benefit as
communication costs exceed the computational work per core. Extremely large core
counts for WRF are defined as those for which *cores > total grid points/10^4*.

*Weak scaling*: When increasing both problem size and core count, the time to
solution remains constant provided that the core count is small enough that the
time spent in I/O and initialization remains negligible. When you increase the
size of the WRF domain, you can usually increase the core count to keep a
constant time to solution—provided that the time spent in I/O and initialization
remains negligible. You will need to run some tests with your own settings
(especially input files and I/O frequency and format) to determine the upper
limit for your core count.

*Strong scaling*: When running WRF on relatively small numbers of cores (*namely
cores < total grid points/10^5*), the time to solution decreases in inverse
proportion to increases in core count if the problem size is unchanged. In such
a strong scaling regime, increasing only the core count has no performance
downside. We recommend making some runs to confirm that you are using the
optimal core count for your problem size.

* Summary
  :PROPERTIES:
  :CUSTOM_ID: summary
  :END:
- openMPI, MPT, and MVAPICH show similar runtime performance.
- intel consistently faster than gnu
